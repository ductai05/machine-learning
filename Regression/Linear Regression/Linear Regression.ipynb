{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45ddl6MkSWxR"
      },
      "source": [
        "## Group 6 - Math for AI, AI23 @ HCMUS\n",
        "- 23122013 - Đinh Đức Tài\n",
        "- 23122002 - Nguyễn Đình Hà Dương\n",
        "- 23122004 - Nguyễn Lê Hoàng Trung\n",
        "- 23122014 - Hoàng Minh Trung\n",
        "\n",
        "## [Lab1] Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQnQG-G5SWxT"
      },
      "source": [
        "## Part 0: Import libs, define DataProcessor and functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGkH95VuSWxT"
      },
      "source": [
        "#### 0.1: Import libs: Numpy, Pandas, Matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:15:40.825158Z",
          "start_time": "2025-04-02T09:15:38.857913Z"
        },
        "id": "UYWnVDGbSWxU"
      },
      "outputs": [],
      "source": [
        "# Importing the libraries: numpy, pandas, matplotlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "\n",
        "from pygments.lexer import include"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jLhOJ_zSWxU"
      },
      "source": [
        "#### 0.2: Create class DataProcessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:15:40.861130Z",
          "start_time": "2025-04-02T09:15:40.850169Z"
        },
        "id": "DI6gspuFSWxV"
      },
      "outputs": [],
      "source": [
        "class DataProcessor:\n",
        "    def __init__(self, file_path):\n",
        "        self.file_path = file_path\n",
        "        self.data = None\n",
        "        self.original = True\n",
        "\n",
        "    def load_data(self, isTest = False):\n",
        "        \"\"\"Load data from the CSV file.\"\"\"\n",
        "        self.data = pd.read_csv(self.file_path)\n",
        "        if (not isTest): print(\"Data loaded successfully!\")\n",
        "        return self.data\n",
        "\n",
        "    def get_summary(self):\n",
        "        \"\"\"Print a summary of the data.\"\"\"\n",
        "        print(\"Number of rows:\", len(self.data))\n",
        "        print(\"Column names:\", self.data.columns.tolist())\n",
        "        return self.data.describe()\n",
        "\n",
        "    def head(self, n = 5):\n",
        "        \"\"\"Return the first n rows of the data.\"\"\"\n",
        "        return self.data.head(n)\n",
        "\n",
        "    def null_info(self):\n",
        "        \"\"\"Print information about missing values.\"\"\"\n",
        "        print(\"\\nNumber of rows with NaN values:\", self.data.isna().any(axis=1).sum())\n",
        "\n",
        "    def get_column_initial_info(self):\n",
        "        print(\"\\nInformation about the columns:\")\n",
        "        column_info = pd.DataFrame({\n",
        "            'Column Name': self.data.columns,\n",
        "            'Description': [\n",
        "                \"Hãng xe\", \"Mẫu xe\", \"Giá xe (VNĐ)\", \"Năm sản xuất\", \"Số km đã đi\",\n",
        "                \"Loại nhiên liệu\", \"Hộp số\", \"Địa điểm bán\", \"Màu xe\", \"Số chủ sở hữu trước đó\",\n",
        "                \"Loại người bán\", \"Dung tích động cơ (cc)\", \"Công suất tối đa (bhp)\",\n",
        "                \"Mô-men xoắn tối đa (Nm)\", \"Hệ dẫn động\", \"Chiều dài xe (mm)\",\n",
        "                \"Chiều rộng xe (mm)\", \"Chiều cao xe (mm)\", \"Số chỗ ngồi\",\n",
        "                \"Dung tích bình nhiên liệu (lít)\"\n",
        "            ],\n",
        "            'Data Type': self.data.dtypes.values,\n",
        "            'Number of NaN': self.data.isna().sum().values,\n",
        "            'Unique Values': self.data.nunique().values,\n",
        "            'Most Frequent Value': self.data.mode().iloc[0].values,\n",
        "        })\n",
        "\n",
        "        return column_info\n",
        "\n",
        "    def get_column_after_transform_info(self):\n",
        "        print(\"\\nInformation about the columns:\")\n",
        "        column_info = pd.DataFrame({\n",
        "            'Column Name': self.data.columns,\n",
        "            'Description': [\n",
        "                \"Hãng xe\", \"Mẫu xe\", \"Giá xe (VNĐ)\", \"Năm sản xuất\", \"Số km đã đi\",\n",
        "                \"Loại nhiên liệu\", \"Hộp số\", \"Địa điểm bán\", \"Màu xe\", \"Số chủ sở hữu trước đó\",\n",
        "                \"Loại người bán\", \"Dung tích động cơ (cc)\", \"Công suất tối đa (bhp)\",\n",
        "                \"Mô-men xoắn tối đa (Nm)\", \"Hệ dẫn động\", \"Chiều dài xe (mm)\",\n",
        "                \"Chiều rộng xe (mm)\", \"Chiều cao xe (mm)\", \"Số chỗ ngồi\",\n",
        "                \"Dung tích bình nhiên liệu (lít)\", 'Vòng tua tại Công suất tối đa (rpm)',\n",
        "                'Vòng tua tại Mô-men xoắn tối đa (rpm)',\n",
        "            ],\n",
        "            'Data Type': self.data.dtypes.values,\n",
        "            'Number of NaN': self.data.isna().sum().values,\n",
        "            'Unique Values': self.data.nunique().values,\n",
        "            'Most Frequent Value': self.data.mode().iloc[0].values,\n",
        "        })\n",
        "\n",
        "        return column_info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcgbagU1SWxW"
      },
      "source": [
        "#### 0.3: Get infomation about unique values functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:15:40.874139Z",
          "start_time": "2025-04-02T09:15:40.867137Z"
        },
        "id": "rZjBM6WrSWxW"
      },
      "outputs": [],
      "source": [
        "def get_some_unique_values(self):\n",
        "    print(\"\\nUnique values of some columns:\")\n",
        "    print(\"Fuel Type:\", self.data['Fuel Type'].unique())\n",
        "    print(\"Transmission:\", self.data['Transmission'].unique())\n",
        "    print(\"Seller Type:\", self.data['Seller Type'].unique())\n",
        "    print(\"Drivetrain:\", self.data['Drivetrain'].unique())\n",
        "\n",
        "    print(\"Owner:\", self.data['Owner'].unique())\n",
        "    print(\"Seating Capacity:\", self.data['Seating Capacity'].unique())\n",
        "\n",
        "def unique_values(self):\n",
        "    object_columns = self.data.select_dtypes(include=['object']).columns\n",
        "    numeric_columns = self.data.select_dtypes(include=['float64', 'int64']).columns\n",
        "\n",
        "    # List of object columns and their unique values\n",
        "    object_columns_list = [(col, self.data[col].nunique()) for col in object_columns]\n",
        "\n",
        "    # List of numeric columns and their unique values\n",
        "    numeric_columns_list = [(col, self.data[col].nunique()) for col in numeric_columns]\n",
        "\n",
        "    print(\"Object Columns and number of unique values: {}\".format(len(object_columns_list)))\n",
        "    print(object_columns_list)\n",
        "\n",
        "    print(\"\\nNumeric Columns and number of unique values: {}\".format(len(numeric_columns_list)))\n",
        "    print(numeric_columns_list)\n",
        "    self.numeric_columns = self.data.select_dtypes(include=['float64', 'int64']).columns\n",
        "    self.object_columns = self.data.select_dtypes(include=['object']).columns\n",
        "    return numeric_columns, object_columns\n",
        "\n",
        "DataProcessor.get_some_unique_values = get_some_unique_values\n",
        "DataProcessor.unique_values = unique_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUFItM3ASWxX"
      },
      "source": [
        "#### 0.4: Clean and transform data functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:15:40.894655Z",
          "start_time": "2025-04-02T09:15:40.884148Z"
        },
        "id": "98-6UkccSWxX"
      },
      "outputs": [],
      "source": [
        "def clean_data(self):\n",
        "    \"\"\"Clean the data by handling missing values and duplicates.\"\"\"\n",
        "    # Handle missing values\n",
        "    # Fill numeric columns with their mean\n",
        "    numeric_columns = self.data.select_dtypes(include=['float64', 'int64']).columns\n",
        "    self.data[numeric_columns] = self.data[numeric_columns].fillna(self.data[numeric_columns].mean().astype(int))\n",
        "\n",
        "    # Fill categorical columns with the most frequent value\n",
        "    categorical_columns = self.data.select_dtypes(include=['object']).columns\n",
        "    self.data[categorical_columns] = self.data[categorical_columns].fillna(self.data[categorical_columns].mode().iloc[0])\n",
        "\n",
        "    # Remove duplicates\n",
        "    self.data = self.data.drop_duplicates()\n",
        "\n",
        "    # Reset index after cleaning\n",
        "    self.data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    return [self.data[numeric_columns].mean().astype(int), self.data[categorical_columns].mode().iloc[0]]\n",
        "\n",
        "def clean_data_test_set(self, info_fill_na):\n",
        "    \"\"\"Clean the data by handling missing values and duplicates.\"\"\"\n",
        "    # Handle missing values\n",
        "    # Fill numeric columns with their mean\n",
        "    numeric_columns = self.data.select_dtypes(include=['float64', 'int64']).columns\n",
        "    self.data[numeric_columns] = self.data[numeric_columns].fillna(info_fill_na[0])\n",
        "\n",
        "    # Fill categorical columns with the most frequent value\n",
        "    categorical_columns = self.data.select_dtypes(include=['object']).columns\n",
        "    self.data[categorical_columns] = self.data[categorical_columns].fillna(info_fill_na[1])\n",
        "\n",
        "    # Reset index after cleaning\n",
        "    self.data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "def transform_data(self):\n",
        "    \"\"\"Transform data by standardizing specific columns.\"\"\"\n",
        "    # 'Engine' ('cc') -> float\n",
        "    self.data['Engine'] = self.data['Engine'].astype(str).str.replace(' cc', '').astype(float)\n",
        "\n",
        "    # Extract RPM values from the original string values before conversion\n",
        "    if (self.original is True):\n",
        "        self.data['rpm at Max Power'] = (\n",
        "            self.data['Max Power']\n",
        "            .astype(str)\n",
        "            .str.extract(r'@\\s*(\\d+)\\s*rpm', expand=False)\n",
        "        )\n",
        "        self.data['rpm at Max Torque'] = (\n",
        "            self.data['Max Torque']\n",
        "            .astype(str)\n",
        "            .str.extract(r'@\\s*(\\d+)\\s*rpm', expand=False)\n",
        "        )\n",
        "    self.original = False\n",
        "\n",
        "    # Fill missing values with the most frequent value\n",
        "    self.data['rpm at Max Power'] = self.data['rpm at Max Power'].fillna(self.data['rpm at Max Power'].mode().iloc[0])\n",
        "    self.data['rpm at Max Torque'] = self.data['rpm at Max Torque'].fillna(self.data['rpm at Max Torque'].mode().iloc[0])\n",
        "\n",
        "    # 'rpm at Max Power' -> int\n",
        "    self.data['rpm at Max Power'] = self.data['rpm at Max Power'].astype(int)\n",
        "\n",
        "    # 'rpm at Max Torque' -> int\n",
        "    self.data['rpm at Max Torque'] = self.data['rpm at Max Torque'].astype(int)\n",
        "\n",
        "    # 'Max Power' ('bhp') -> int\n",
        "    self.data['Max Power'] = self.data['Max Power'].astype(str).str.extract(r'(\\d+)', expand=False).astype(int)\n",
        "\n",
        "    # 'Max Torque' ('Nm') -> int\n",
        "    self.data['Max Torque'] = self.data['Max Torque'].astype(str).str.extract(r'(\\d+)', expand=False).astype(int)\n",
        "\n",
        "    # 'Seating Capacity' -> int\n",
        "    self.data['Seating Capacity'] = self.data['Seating Capacity'].astype(int)\n",
        "\n",
        "    # 'Fuel Tank Capacity' -> int\n",
        "    self.data['Fuel Tank Capacity'] = self.data['Fuel Tank Capacity'].astype(int)\n",
        "\n",
        "    # 'Owner' -> int\n",
        "    self.data['Owner'] = self.data['Owner'].map({\n",
        "        'UnRegistered Car': 0,\n",
        "        'First': 1,\n",
        "        'Second': 2,\n",
        "        'Third': 3,\n",
        "        'Fourth': 4,\n",
        "        '4 or More': 5,\n",
        "        0: 0,\n",
        "        1: 1,\n",
        "        2: 2,\n",
        "        3: 3,\n",
        "        4: 4,\n",
        "        5: 5,\n",
        "    })\n",
        "\n",
        "    self.numeric_columns = self.data.select_dtypes(include=['float64', 'int64']).columns\n",
        "    self.object_columns = self.data.select_dtypes(include=['object']).columns\n",
        "\n",
        "DataProcessor.clean_data = clean_data\n",
        "DataProcessor.clean_data_test_set = clean_data_test_set\n",
        "DataProcessor.transform_data = transform_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMieWYTUSWxY"
      },
      "source": [
        "#### 0.5: Data visualization functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:15:40.926994Z",
          "start_time": "2025-04-02T09:15:40.913659Z"
        },
        "id": "JF9gEyF_SWxY"
      },
      "outputs": [],
      "source": [
        "def plot_corr_matrix(self, width=12, height=8):\n",
        "    # Compute the correlation matrix using only numeric features\n",
        "    corr_matrix = self.data[self.numeric_columns].corr()\n",
        "\n",
        "    # Plot the correlation matrix using matplotlib\n",
        "    plt.figure(figsize=(width, height))\n",
        "    plt.imshow(corr_matrix, cmap='coolwarm', interpolation='none')\n",
        "    plt.colorbar()\n",
        "    plt.xticks(range(len(corr_matrix)), corr_matrix.columns, rotation=90)\n",
        "    plt.yticks(range(len(corr_matrix)), corr_matrix.columns)\n",
        "    plt.title(\"Correlation Matrix of Numeric Features\")\n",
        "\n",
        "    # Annotate the matrix with correlation coefficients\n",
        "    for i in range(len(corr_matrix)):\n",
        "        for j in range(len(corr_matrix)):\n",
        "            plt.text(j, i, f\"{corr_matrix.iloc[i, j]:.2f}\", ha='center', va='center', color='black')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_distribution_of_numeric_columns(self):\n",
        "    # Plot histograms for all numeric columns\n",
        "    num_cols = self.numeric_columns\n",
        "\n",
        "    num_cols_count = len(num_cols)\n",
        "    n_cols = 3  # Number of columns in the figure\n",
        "    n_rows = (num_cols_count + n_cols - 1) // n_cols  # Calculate the number of rows needed\n",
        "\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 3.5 * n_rows))\n",
        "    axes = axes.flatten()  # Flatten the axes array for easy iteration\n",
        "\n",
        "    for i, col in enumerate(num_cols):\n",
        "        axes[i].hist(self.data[col], bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
        "        axes[i].set_title(f'Histogram of {col}')\n",
        "        axes[i].set_xlabel(col)\n",
        "        axes[i].set_ylabel('Frequency')\n",
        "        axes[i].grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Hide any unused subplots\n",
        "    for j in range(i + 1, len(axes)):\n",
        "        fig.delaxes(axes[j])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def box_plot_for_object_columns(self):\n",
        "    for col in self.object_columns:\n",
        "        if (col == 'Model'):\n",
        "            continue\n",
        "        plt.figure(figsize=(12, 3))\n",
        "        categories = self.data[col].unique()\n",
        "        groups = [self.data.loc[self.data[col] == category, 'Price'] for category in categories]\n",
        "        plt.boxplot(groups, patch_artist=True, tick_labels=categories)\n",
        "        plt.title(f'Box Plot: Price by {col}')\n",
        "        plt.xlabel(col)\n",
        "        plt.ylabel('Price')\n",
        "        if (col == 'Location'):\n",
        "            plt.xticks(rotation=90)\n",
        "        elif (col == 'Fuel Type' or col == 'Transmission' or col == 'Seller Type' or col == 'Drivetrain'):\n",
        "            plt.xticks(rotation=0)\n",
        "        else:\n",
        "            plt.xticks(rotation=60)\n",
        "        plt.show()\n",
        "\n",
        "def scatter_plot_for_numeric_columns(self):\n",
        "    cols = [col for col in self.numeric_columns if col != 'Price']\n",
        "    n_plots = len(cols)\n",
        "    n_cols = 3\n",
        "    n_rows = int(n_plots / n_cols) + 1\n",
        "\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 4 * n_rows))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, col in enumerate(cols):\n",
        "        axes[i].scatter(self.data[col], self.data['Price'], alpha=0.5, color='blue', edgecolors='k')\n",
        "        axes[i].set_title(f'Relationship between Price and {col}')\n",
        "        axes[i].set_xlabel(col)\n",
        "        axes[i].set_ylabel('Price')\n",
        "        axes[i].grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Remove any unused subplots\n",
        "    for j in range(i + 1, len(axes)):\n",
        "        fig.delaxes(axes[j])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "DataProcessor.plot_corr_matrix = plot_corr_matrix\n",
        "DataProcessor.plot_distribution_of_numeric_columns = plot_distribution_of_numeric_columns\n",
        "DataProcessor.box_plot_for_object_columns = box_plot_for_object_columns\n",
        "DataProcessor.scatter_plot_for_numeric_columns = scatter_plot_for_numeric_columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUgXdZUsSWxY"
      },
      "source": [
        "#### 0.6: Split dataset to train set and validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:15:41.486083Z",
          "start_time": "2025-04-02T09:15:41.478488Z"
        },
        "id": "2A2H45SCSWxZ"
      },
      "outputs": [],
      "source": [
        "def train_valid_split(self, valid_size=0.2, random_state=None):\n",
        "    \"\"\"Split the data into training and validation sets, ensuring all unique 'Model' values are present in the training set.\"\"\"\n",
        "    train_set = copy.deepcopy(self)\n",
        "    valid_set = copy.deepcopy(self)\n",
        "\n",
        "    # Set random seed if provided\n",
        "    if random_state is not None:\n",
        "        np.random.seed(random_state)\n",
        "\n",
        "    # Identify unique models\n",
        "    unique_models = self.data['Model'].unique()\n",
        "\n",
        "    # Create training indices: start with an empty list\n",
        "    train_indices = []\n",
        "\n",
        "    # Iterate through each unique model\n",
        "    for model in unique_models:\n",
        "        # Get indices where the 'Model' column equals the current unique model\n",
        "        model_indices = self.data[self.data['Model'] == model].index.tolist()\n",
        "\n",
        "        # Randomly choose one index for each model to be included in the training set\n",
        "        # This ensures that at least one sample of each model is in the training set\n",
        "        chosen_index = np.random.choice(model_indices)\n",
        "        train_indices.append(chosen_index)\n",
        "\n",
        "    # Create remaining indices for the training set\n",
        "    remaining_count = int((1 - valid_size) * len(self.data)) - len(train_indices)\n",
        "    remaining_indices = list(set(np.arange(len(self.data))) - set(train_indices))\n",
        "\n",
        "    # Randomly sample from the remaining indices to complete the training set\n",
        "    sampled_indices = np.random.choice(remaining_indices, size=remaining_count, replace=False).tolist()\n",
        "    train_indices.extend(sampled_indices)\n",
        "\n",
        "    # Create validation indices: the indices not in the training set\n",
        "    valid_indices = list(set(np.arange(len(self.data))) - set(train_indices))\n",
        "\n",
        "    # Shuffle the training indices\n",
        "    np.random.shuffle(train_indices)\n",
        "\n",
        "    # Assign data to train_set and valid_set\n",
        "    train_set.data = self.data.iloc[train_indices].reset_index(drop=True)\n",
        "    valid_set.data = self.data.iloc[valid_indices].reset_index(drop=True)\n",
        "\n",
        "    return train_set, valid_set\n",
        "\n",
        "DataProcessor.train_valid_split = train_valid_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qf5HKxyvSWxZ"
      },
      "source": [
        "#### 0.7: Data Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:15:41.505971Z",
          "start_time": "2025-04-02T09:15:41.492944Z"
        },
        "id": "VcTmDDo9SWxZ"
      },
      "outputs": [],
      "source": [
        "def encode_data(self):\n",
        "    \"\"\"\n",
        "    Mã hóa các biến theo yêu cầu:\n",
        "    - Hãng xe (Make): thay thế bằng giá trung vị của các xe thuộc hãng đó\n",
        "    - Mẫu xe (Model): thay thế bằng giá trung vị của các xe cùng mẫu\n",
        "    - Loại nhiên liệu (Fuel Type): Hybrid -> 1; Ngược lại -> 0\n",
        "    - Hộp số (Transmission): Auto -> 1; Manual -> 0\n",
        "    - Địa điểm (Location): nếu giá trung vị của Location ≥ giá trung vị toàn cục -> 1; ngược lại -> 0\n",
        "    - Màu sắc (Color): Black, Blue, Yellow -> 1; ngược lại -> 0\n",
        "    - Loại người bán (Seller Type): Corporate -> 1; Individual và Commercial Registration -> 0\n",
        "    - Drivetrain: FWD -> 1, RWD -> 2, AWD -> 3\n",
        "    \"\"\"\n",
        "    # Hãng xe: map giá trung vị theo Make\n",
        "    make_median = self.data.groupby('Make')['Price'].median()\n",
        "    self.data['Make'] = self.data['Make'].map(make_median)\n",
        "\n",
        "    # Mẫu xe: map giá trung vị theo Model\n",
        "    model_median = self.data.groupby('Model')['Price'].median()\n",
        "    self.data['Model'] = self.data['Model'].map(model_median)\n",
        "\n",
        "    # Loại nhiên liệu: chỉ giữ Hybrid = 1, còn lại = 0\n",
        "    self.data['Fuel Type'] = self.data['Fuel Type'].apply(lambda x: 1 if x.strip().lower() == 'hybrid' else 0)\n",
        "\n",
        "    # Hộp số: Auto = 1; Manual = 0\n",
        "    self.data['Transmission'] = self.data['Transmission'].apply(lambda x: 1 if 'auto' in x.strip().lower() else 0)\n",
        "\n",
        "    # Địa điểm: chia theo giá trung vị so với toàn bộ data\n",
        "    global_median_price = self.data['Price'].median()\n",
        "    location_medians = self.data.groupby('Location')['Price'].median()\n",
        "    def encode_location(loc):\n",
        "        return 1 if location_medians.loc[loc] >= global_median_price else 0\n",
        "    self.data['Location'] = self.data['Location'].apply(encode_location)\n",
        "\n",
        "    # Màu sắc: Black, Blue, Yellow -> 1; khác -> 0\n",
        "    self.data['Color'] = self.data['Color'].apply(lambda x: 1 if x.strip().lower() in ['black', 'blue', 'yellow'] else 0)\n",
        "\n",
        "    # Loại người bán: Corporate = 1; Individual và Commercial Registration = 0\n",
        "    self.data['Seller Type'] = self.data['Seller Type'].apply(lambda x: 1 if x.strip().lower() == 'corporate' else 0)\n",
        "\n",
        "    # Drivetrain: FWD = 1, RWD = 2, AWD = 3\n",
        "    drivetrain_mapping = {'FWD': 1, 'RWD': 2, 'AWD': 3}\n",
        "    self.data['Drivetrain'] = self.data['Drivetrain'].map(drivetrain_mapping)\n",
        "\n",
        "    # Save numeric and object columns\n",
        "    self.numeric_columns = self.data.select_dtypes(include=['float64', 'int64']).columns\n",
        "    self.object_columns = self.data.select_dtypes(include=['object']).columns\n",
        "\n",
        "    # Save encoding information for test set\n",
        "    encoding_information = {}\n",
        "    encoding_information['make_median'] = make_median\n",
        "    encoding_information['model_median'] = model_median\n",
        "    encoding_information['location_medians'] = location_medians\n",
        "    encoding_information['global_median_price'] = global_median_price\n",
        "    encoding_information['drivetrain_mapping'] = drivetrain_mapping\n",
        "\n",
        "    return encoding_information\n",
        "\n",
        "# Gắn hàm encode_data vào class DataProcessor\n",
        "DataProcessor.encode_data = encode_data\n",
        "\n",
        "def encode_data_test_set(self, encoding_information):\n",
        "    \"\"\"\n",
        "    Mã hóa dữ liệu kiểm tra theo thông tin đã học từ tập huấn luyện:\n",
        "    - Hãng xe (Make): thay thế bằng giá trung vị của các xe thuộc hãng đó\n",
        "    - Mẫu xe (Model): thay thế bằng giá trung vị của các xe cùng mẫu\n",
        "    - Loại nhiên liệu (Fuel Type): Hybrid -> 1; Ngược lại -> 0\n",
        "    - Hộp số (Transmission): Auto -> 1; Manual -> 0\n",
        "    - Địa điểm (Location): nếu giá trung vị của Location ≥ giá trung vị toàn cục -> 1; ngược lại -> 0\n",
        "    - Màu sắc (Color): Black, Blue, Yellow -> 1; ngược lại -> 0\n",
        "    - Loại người bán (Seller Type): Corporate -> 1; Individual và Commercial Registration -> 0\n",
        "    - Drivetrain: FWD -> 1, RWD -> 2, AWD -> 3\n",
        "    \"\"\"\n",
        "    # Hãng xe: map giá trung vị theo Make, use global_median_price for missing keys\n",
        "    self.data['Make'] = self.data['Make'].apply(\n",
        "        lambda x: encoding_information['make_median'].get(x, encoding_information['global_median_price'])\n",
        "    )\n",
        "\n",
        "    # Mẫu xe: map giá trung vị theo Model, use global_median_price for missing keys\n",
        "    self.data['Model'] = self.data['Model'].apply(\n",
        "        lambda x: encoding_information['model_median'].get(x, encoding_information['global_median_price'])\n",
        "    )\n",
        "    # Loại nhiên liệu: chỉ giữ Hybrid = 1, còn lại = 0\n",
        "    self.data['Fuel Type'] = self.data['Fuel Type'].apply(lambda x: 1 if x.strip().lower() == 'hybrid' else 0)\n",
        "\n",
        "    # Hộp số: Auto = 1; Manual = 0\n",
        "    self.data['Transmission'] = self.data['Transmission'].apply(lambda x: 1 if 'auto' in x.strip().lower() else 0)\n",
        "\n",
        "    # Địa điểm: chia theo giá trung vị so với toàn bộ data\n",
        "    global_median_price = encoding_information['global_median_price']\n",
        "    location_medians = encoding_information['location_medians']\n",
        "    def encode_location(loc):\n",
        "        try:\n",
        "            return 1 if location_medians.loc[loc] >= global_median_price else 0\n",
        "        except KeyError:\n",
        "            return 0\n",
        "    self.data['Location'] = self.data['Location'].apply(encode_location)\n",
        "\n",
        "    # Màu sắc: Black, Blue, Yellow -> 1; khác -> 0\n",
        "    self.data['Color'] = self.data['Color'].apply(lambda x: 1 if x.strip().lower() in ['black', 'blue', 'yellow'] else 0)\n",
        "\n",
        "    # Loại người bán: Corporate = 1; Individual và Commercial Registration = 0\n",
        "    self.data['Seller Type'] = self.data['Seller Type'].apply(lambda x: 1 if x.strip().lower() == 'corporate' else 0)\n",
        "\n",
        "    # Drivetrain: FWD = 1, RWD = 2, AWD = 3\n",
        "    drivetrain_mapping = encoding_information['drivetrain_mapping']\n",
        "    self.data['Drivetrain'] = self.data['Drivetrain'].map(drivetrain_mapping)\n",
        "\n",
        "    # Save numeric and object columns\n",
        "    self.numeric_columns = self.data.select_dtypes(include=['float64', 'int64']).columns\n",
        "    self.object_columns = self.data.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Gắn hàm encode_data_test_set vào class DataProcessor\n",
        "DataProcessor.encode_data_test_set = encode_data_test_set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YEEDZeVSWxZ"
      },
      "source": [
        "#### 0.8: Data Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:15:41.520120Z",
          "start_time": "2025-04-02T09:15:41.514127Z"
        },
        "id": "KiP75eJbSWxZ"
      },
      "outputs": [],
      "source": [
        "def normalize_data(self):\n",
        "    \"\"\"\n",
        "    Chuẩn hóa các feature dữ liệu:\n",
        "    - Áp dụng log transformation cho 'Price' và 'Kilometer' nhằm giảm ảnh hưởng do độ lệch quy mô.\n",
        "    - Sau đó dùng MinMaxScaler chuẩn hóa toàn bộ các biến số.\"\n",
        "    \"\"\"\n",
        "\n",
        "    # Áp dụng log1p (log(1+x)) để tránh lỗi với giá trị 0\n",
        "    self.data['Kilometer'] = np.log1p(self.data['Kilometer'])\n",
        "\n",
        "    # Chọn các cột số để scale\n",
        "    numeric_cols = self.data.select_dtypes(include=['float64', 'int64']).columns\n",
        "    numeric_cols = numeric_cols.drop(['Price'])\n",
        "\n",
        "    min_vals = self.data[numeric_cols].min()\n",
        "    max_vals = self.data[numeric_cols].max()\n",
        "    range_vals = max_vals - min_vals\n",
        "    # Avoid division by zero by replacing 0 differences with 1\n",
        "    range_vals[range_vals == 0] = 1\n",
        "    self.data[numeric_cols] = (self.data[numeric_cols] - min_vals) / range_vals\n",
        "\n",
        "    return range_vals, min_vals, max_vals\n",
        "\n",
        "DataProcessor.normalize_data = normalize_data\n",
        "\n",
        "def normalize_data_test_set(self, range_vals, min_vals):\n",
        "    \"\"\"\n",
        "    Chuẩn hóa dữ liệu kiểm tra theo thông tin đã học từ tập huấn luyện:\n",
        "    - Áp dụng log transformation cho 'Price' và 'Kilometer' nhằm giảm ảnh hưởng do độ lệch quy mô.\n",
        "    - Sau đó dùng MinMaxScaler chuẩn hóa toàn bộ các biến số.\"\n",
        "    \"\"\"\n",
        "    # Áp dụng log1p (log(1+x)) để tránh lỗi với giá trị 0\n",
        "    self.data['Kilometer'] = np.log1p(self.data['Kilometer'])\n",
        "\n",
        "    # Chọn các cột số để scale\n",
        "    numeric_cols = self.data.select_dtypes(include=['float64', 'int64']).columns\n",
        "    numeric_cols = numeric_cols.drop(['Price'])\n",
        "\n",
        "    # Normalize the test set\n",
        "    self.data[numeric_cols] = (self.data[numeric_cols] - min_vals) / range_vals\n",
        "\n",
        "DataProcessor.normalize_data_test_set = normalize_data_test_set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieMN6vN_SWxa"
      },
      "source": [
        "## Part I: Load and Explore data (train.csv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IQUDjmGSWxa"
      },
      "source": [
        "In this part, we will load and explore some information about the original data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:15:41.729989Z",
          "start_time": "2025-04-02T09:15:41.676282Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "oMdy_hhnSWxa",
        "outputId": "c110af41-cecd-43e1-b94b-6976752b4489"
      },
      "outputs": [],
      "source": [
        "# Initialize the DataProcessor class and load the data (train.csv)\n",
        "file_path = './data/train.csv'\n",
        "data = DataProcessor(file_path)\n",
        "data.load_data()\n",
        "\n",
        "# Print summary of the data\n",
        "print(\"\\nSummary of the data:\")\n",
        "data.get_summary()\n",
        "\n",
        "# First 5 rows of data\n",
        "print(\"\\nFirst 5 rows of data:\")\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:15:41.963357Z",
          "start_time": "2025-04-02T09:15:41.941517Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "y4X2oKbPSWxa",
        "outputId": "8fde11e6-8896-45b7-8918-b9cc29afaeef"
      },
      "outputs": [],
      "source": [
        "# Print information about the null values\n",
        "data.null_info()\n",
        "\n",
        "# Print information about the columns\n",
        "data.get_column_initial_info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:15:42.284848Z",
          "start_time": "2025-04-02T09:15:42.278199Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4CPla41SWxb",
        "outputId": "16780bf2-61c8-4dff-e0f1-583d237d265b"
      },
      "outputs": [],
      "source": [
        "# Print the unique values of some columns\n",
        "data.get_some_unique_values()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "so357rYrSWxb"
      },
      "source": [
        "## Part II: Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJnYmCnZSWxb"
      },
      "source": [
        "In this part, we split data into a training set and a validation set, fill NaN values, and perform some data transformation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVOzzKbtSWxb"
      },
      "source": [
        "#### II.0: Split dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:15:43.023626Z",
          "start_time": "2025-04-02T09:15:42.726165Z"
        },
        "id": "w8rDJAxZSWxb"
      },
      "outputs": [],
      "source": [
        "train_data, valid_data = data.train_valid_split(valid_size=0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f4l5xd1SWxb"
      },
      "source": [
        "#### II.1: Data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:15:43.282141Z",
          "start_time": "2025-04-02T09:15:43.245799Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KimHUOWbSWxb",
        "outputId": "f657123d-d697-4526-b4bd-eae44a5c6f25"
      },
      "outputs": [],
      "source": [
        "# Clean the data and fill missing values\n",
        "info_fill_na = train_data.clean_data()\n",
        "valid_data.clean_data_test_set(info_fill_na)\n",
        "\n",
        "# Print summary after cleaning\n",
        "print(\"Data cleaned successfully!\")\n",
        "print(\"Number of rows after cleaning:\", len(train_data.data))\n",
        "print(\"Number of missing values after cleaning:\", train_data.data.isna().sum().sum())\n",
        "\n",
        "print(\"Data cleaned successfully!\")\n",
        "print(\"Number of rows after cleaning:\", len(valid_data.data))\n",
        "print(\"Number of missing values after cleaning:\", valid_data.data.isna().sum().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsMtM3yfSWxc"
      },
      "source": [
        "#### II.2: Data Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:15:43.447742Z",
          "start_time": "2025-04-02T09:15:43.418147Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cALuoKjlSWxc",
        "outputId": "12416c08-1d21-49bf-c131-74a92b357b4a"
      },
      "outputs": [],
      "source": [
        "# Transform the data\n",
        "train_data.transform_data()\n",
        "valid_data.transform_data()\n",
        "\n",
        "print(\"Data transformed successfully!\")\n",
        "print(\"\\nData Transformation Details:\")\n",
        "print(\"- 'Engine' (cc) converted to float.\")\n",
        "print(\"- 'Max Power' (bhp) converted to integer.\")\n",
        "print(\"- 'Max Torque' (Nm) converted to integer.\")\n",
        "print(\"- Add 'rpm at Max Power' and converted to integer.\")\n",
        "print(\"- Add 'rpm at Max Torque' and converted to integer.\")\n",
        "print(\"- 'Seating Capacity' converted to integer.\")\n",
        "print(\"- 'Fuel Tank Capacity' converted to integer.\")\n",
        "print(\"- 'Owner' converted to numerical categories.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1wvSQYaSWxc"
      },
      "source": [
        "#### II.3: Explore train data after preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:15:43.689274Z",
          "start_time": "2025-04-02T09:15:43.668474Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "VyMTOkCHSWxc",
        "outputId": "1ac76726-e8df-405f-e9e4-d8eccc6886d6"
      },
      "outputs": [],
      "source": [
        "train_data.get_column_after_transform_info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:15:43.927242Z",
          "start_time": "2025-04-02T09:15:43.907733Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ADePzdqHSWxc",
        "outputId": "65b75e4f-0faa-48d6-c720-88f4c284a501"
      },
      "outputs": [],
      "source": [
        "valid_data.get_column_after_transform_info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:15:44.405392Z",
          "start_time": "2025-04-02T09:15:44.394840Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdzAan3cSWxd",
        "outputId": "559a9bfd-1bf3-41bf-f19d-1ebe25a95c3a"
      },
      "outputs": [],
      "source": [
        "# Find models in validation set that are not in train set\n",
        "train_models = train_data.data['Model'].unique()\n",
        "validation_models = valid_data.data['Model'].unique()\n",
        "\n",
        "new_models = [model for model in validation_models if model not in train_models]\n",
        "\n",
        "print(\"Models in validation set but not in train set:\", new_models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:15:44.753303Z",
          "start_time": "2025-04-02T09:15:44.746758Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zP7cCsAtSWxd",
        "outputId": "b469dcee-6b0c-49ae-e0f2-7773e93af2c3"
      },
      "outputs": [],
      "source": [
        "train_data.get_some_unique_values()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:15:45.092193Z",
          "start_time": "2025-04-02T09:15:45.081240Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cEBKlu1SWxd",
        "outputId": "9f173f5d-bc24-4591-bf33-4077de97b54b"
      },
      "outputs": [],
      "source": [
        "# Print number of unique values for each column\n",
        "numeric_columns, object_columns = train_data.unique_values()\n",
        "print()\n",
        "numeric_columns, object_columns = valid_data.unique_values()\n",
        "\n",
        "# Create save point: after preprocessing\n",
        "train_data_after_preprocessing = copy.deepcopy(train_data)\n",
        "validation_data_after_preprocessing = copy.deepcopy(valid_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRxWR0zbSWxd"
      },
      "source": [
        "## Part III: Data visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:15:46.307088Z",
          "start_time": "2025-04-02T09:15:45.503358Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "6_T6886XSWxd",
        "outputId": "97766cbb-91cc-4cc6-84a0-23cbbd0bc313"
      },
      "outputs": [],
      "source": [
        "# 1. Plot the correlation matrix\n",
        "train_data.plot_corr_matrix()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:15:49.809691Z",
          "start_time": "2025-04-02T09:15:46.423847Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Z7yFxmUWSWxm",
        "outputId": "13572d12-d96a-4e80-f78a-76716c3c98fb"
      },
      "outputs": [],
      "source": [
        "# 2. Plot the distribution of numeric columns\n",
        "train_data.plot_distribution_of_numeric_columns()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:15:52.060204Z",
          "start_time": "2025-04-02T09:15:50.149172Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "mHEkwZS-SWxm",
        "outputId": "aa2b0802-b9ab-4a43-e500-a80260d7dbf4"
      },
      "outputs": [],
      "source": [
        "# 3. Box plot: Relationship between Price and Object Columns\n",
        "train_data.box_plot_for_object_columns()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:15:54.798467Z",
          "start_time": "2025-04-02T09:15:52.093834Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "FgrSZbXuSWxn",
        "outputId": "0c23d368-e231-42ba-983c-9922c31d0dca"
      },
      "outputs": [],
      "source": [
        "# 4. Scatter Plot: Relationship between Price and Numeric Columns\n",
        "train_data.scatter_plot_for_numeric_columns()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehyyfh6vSWxn"
      },
      "source": [
        "## Part IV: Data Encoding and Data Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgtYFiZISWxn"
      },
      "source": [
        "#### IV.1: Train set and Validation set\n",
        "We will use train data and validation data (after preprocessing)\n",
        "- train_data_after_preprocessing\n",
        "- validation_data_after_preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:15:55.655520Z",
          "start_time": "2025-04-02T09:15:55.642474Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmtX7gTkSWxn",
        "outputId": "02a7a388-95e2-4eeb-fcb6-d55e14e0fcfa"
      },
      "outputs": [],
      "source": [
        "train_data = copy.deepcopy(train_data_after_preprocessing) # after preprocessing\n",
        "numeric_columns, object_columns = train_data.unique_values()\n",
        "print()\n",
        "validation_data = copy.deepcopy(validation_data_after_preprocessing) # after preprocessing\n",
        "numeric_columns, object_columns = validation_data.unique_values()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvM5mF-tSWxn"
      },
      "source": [
        "#### IV.2: Data Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:15:55.937045Z",
          "start_time": "2025-04-02T09:15:55.892586Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "9R2KpYu8SWxn",
        "outputId": "31e956ab-2567-490d-bd7a-19ddeed6acb3"
      },
      "outputs": [],
      "source": [
        "# Encode the training data\n",
        "encoding_information = train_data.encode_data()\n",
        "print(\"Data encoding completed!\")\n",
        "train_data.get_column_after_transform_info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:15:56.153366Z",
          "start_time": "2025-04-02T09:15:56.121675Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "D8GbD91XSWxn",
        "outputId": "000d4afd-de33-4029-dc1d-dc0dda592510"
      },
      "outputs": [],
      "source": [
        "# Encode the validation data\n",
        "validation_data.encode_data_test_set(encoding_information)\n",
        "print(\"Data encoding completed!\")\n",
        "validation_data.get_column_after_transform_info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:15:56.488554Z",
          "start_time": "2025-04-02T09:15:56.483768Z"
        },
        "id": "uu1kRMKaSWxn"
      },
      "outputs": [],
      "source": [
        "# Create save point: after encoding\n",
        "train_data_after_encoding = copy.deepcopy(train_data)\n",
        "validation_data_after_encoding = copy.deepcopy(validation_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMM5seH8SWxo"
      },
      "source": [
        "#### IV.3: Data Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfbrXj5mSWxo"
      },
      "source": [
        "##### Training set normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:15:57.097473Z",
          "start_time": "2025-04-02T09:15:57.055636Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "r1HnP2rxSWxo",
        "outputId": "de342f73-4c68-41f1-db05-2ae1a9ec4d37"
      },
      "outputs": [],
      "source": [
        "train_data = copy.deepcopy(train_data_after_encoding)\n",
        "train_data.data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:15:57.445040Z",
          "start_time": "2025-04-02T09:15:57.390238Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "uJsW-Df6SWxo",
        "outputId": "f76991ea-85e4-4b84-8308-9235dc52737b"
      },
      "outputs": [],
      "source": [
        "range_vals, min_vals, max_vals = train_data.normalize_data()\n",
        "train_data_after_normalization = copy.deepcopy(train_data) # after normalization\n",
        "print(\"Data normalization completed!\")\n",
        "train_data_after_normalization.data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:15:57.922691Z",
          "start_time": "2025-04-02T09:15:57.917959Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdJpyDOlSWxo",
        "outputId": "63dd178d-6521-4ef3-e181-1fbe4c80e449"
      },
      "outputs": [],
      "source": [
        "range_vals, min_vals, max_vals\n",
        "# print range_vals, min_vals, max_vals  size\n",
        "print(range_vals.size, min_vals.size, max_vals.size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbxRBRAUSWxp"
      },
      "source": [
        "##### Validation set normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:15:58.408258Z",
          "start_time": "2025-04-02T09:15:58.368971Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "TAzHt9kXSWxp",
        "outputId": "80097767-e9f7-49c6-d85c-28dfbfec1c63"
      },
      "outputs": [],
      "source": [
        "validation_data = copy.deepcopy(validation_data_after_encoding)\n",
        "print(\"Data normalization completed!\")\n",
        "validation_data.data.describe()\n",
        "validation_data.data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:15:59.081386Z",
          "start_time": "2025-04-02T09:15:59.033880Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "CqEGGRMiSWxp",
        "outputId": "76f47d3f-6ec6-4432-ce82-5a5dedae3f92"
      },
      "outputs": [],
      "source": [
        "validation_data.normalize_data_test_set(range_vals, min_vals)\n",
        "validation_data_after_normalization = copy.deepcopy(validation_data) # after normalization\n",
        "validation_data_after_normalization.data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:16:00.656557Z",
          "start_time": "2025-04-02T09:15:59.469583Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "i3OWoZA1SWxp",
        "outputId": "8fb145bb-db62-4972-d07c-0b45635c1d78"
      },
      "outputs": [],
      "source": [
        "train_data_after_normalization.plot_corr_matrix(15, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:16:02.813161Z",
          "start_time": "2025-04-02T09:16:01.506533Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "YwnqRl34SWxp",
        "outputId": "2b1e1510-b51d-456f-d52e-c965e316cc7a"
      },
      "outputs": [],
      "source": [
        "validation_data_after_normalization.plot_corr_matrix(15, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtPzjqSYSWxp"
      },
      "source": [
        "## Part V: Split dataset and tools to evaluate models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEG1muruSWxp"
      },
      "source": [
        "#### V.1: Split dataset\n",
        "Split training set into X_train and y_train. Split validation set into X_val and y_val."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:16:03.052550Z",
          "start_time": "2025-04-02T09:16:03.043250Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWxHmMKgSWxq",
        "outputId": "70850674-1444-4059-c704-79ed1a109af7"
      },
      "outputs": [],
      "source": [
        "train_data = copy.deepcopy(train_data_after_normalization)\n",
        "validation_data = copy.deepcopy(validation_data_after_normalization)\n",
        "numeric_columns, object_columns = train_data.numeric_columns, train_data.object_columns\n",
        "print(numeric_columns, len(numeric_columns))\n",
        "\n",
        "train_data = train_data.data\n",
        "validation_data = validation_data.data\n",
        "\n",
        "# X_train, y_train from train_data\n",
        "# X_val, y_val from validation_data\n",
        "\n",
        "X_train = train_data.drop(columns=['Price'])\n",
        "y_train = train_data['Price']\n",
        "\n",
        "X_val = validation_data.drop(columns=['Price'])\n",
        "y_val = validation_data['Price']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMiTjc11SWxq"
      },
      "source": [
        "#### V.2: Tools to evaluate the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:16:03.390144Z",
          "start_time": "2025-04-02T09:16:03.383556Z"
        },
        "id": "fCoGnWxVSWxq"
      },
      "outputs": [],
      "source": [
        "# Function to evaluate the model\n",
        "def mse(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return np.sqrt(mse(y_true, y_pred))\n",
        "\n",
        "def mae(y_true, y_pred):\n",
        "    return np.mean(np.abs(y_true - y_pred))\n",
        "\n",
        "def r2_score(y_true, y_pred):\n",
        "    SSE = np.sum((y_true - y_pred) ** 2)\n",
        "    SST = np.sum((y_true - np.mean(y_true)) ** 2)\n",
        "    return 1 - (SSE / SST) # R^2 = 1 - SSE/SST = SSR/SST\n",
        "\n",
        "def evaluate_model(y_true, y_pred):\n",
        "    model_eval = pd.DataFrame({\n",
        "        'Metric': ['Mean Squared Error', 'Root Mean Squared Error', 'Mean Absolute Error', 'R^2 Score'],\n",
        "        'Value': [mse(y_true, y_pred), rmse(y_true, y_pred), mae(y_true, y_pred), r2_score(y_true, y_pred)]\n",
        "    })\n",
        "    return model_eval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUfhmMVmSWxq"
      },
      "source": [
        "## Part VI: Simple Linear Regression model from Statistics's point of view"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZ7tb6IUSWxq"
      },
      "source": [
        "Based on Correlation coefficient map, **Model** maybe the best feature for model. We will prove it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:16:03.687079Z",
          "start_time": "2025-04-02T09:16:03.623538Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozGlGmQWSWxq",
        "outputId": "0e80aaee-91eb-4476-c71b-234bdfbf1a3c"
      },
      "outputs": [],
      "source": [
        "# Y = beta0 + beta1*X\n",
        "# beta1 = Sxx / Sxy\n",
        "# beta0 = mean(Y) - beta1 * mean(X)\n",
        "# Sxx = sum((X - mean(X))^2)\n",
        "# Sxy = sum((X - mean(X)) * (Y - mean(Y)))\n",
        "\n",
        "def simple_linear_regression(X, y):\n",
        "    mean_X = np.mean(X)\n",
        "    mean_y = np.mean(y)\n",
        "    Sxx = np.sum((X - mean_X) ** 2)\n",
        "    Sxy = np.sum((X - mean_X) * (y - mean_y))\n",
        "    beta1 = Sxy / Sxx\n",
        "    beta0 = mean_y - beta1 * mean_X\n",
        "    return beta0, beta1\n",
        "\n",
        "result = []\n",
        "\n",
        "for F in numeric_columns.drop(['Price']):\n",
        "    # Predict the price of a car based on feature F\n",
        "    beta0, beta1 = simple_linear_regression(X_train[F], y_train)\n",
        "    # print(f\"{F} feature: Price = {beta0:.2f} + {beta1:.2f} * {F}\")\n",
        "\n",
        "    # Predict the price of a car based on feature F\n",
        "    y_pred = beta0 + beta1 * X_train[F]\n",
        "\n",
        "    # Evaluate the model on train set\n",
        "    model_eval = evaluate_model(y_train, y_pred)\n",
        "    result.append([F, model_eval, beta0, beta1])\n",
        "\n",
        "Metric = ['Mean Squared Error', 'Root Mean Squared Error', 'Mean Absolute Error', 'R^2 Score']\n",
        "\n",
        "for i, metric in enumerate(Metric):\n",
        "    best = result[0]\n",
        "    for res in result:\n",
        "        if (res[1].iloc[i, 1] < best[1].iloc[i, 1] and metric != 'R^2 Score') or \\\n",
        "           (res[1].iloc[i, 1] > best[1].iloc[i, 1] and metric == 'R^2 Score'):\n",
        "            best = res\n",
        "    print(f\"Best feature for {metric}: {best[0]}. Value: {best[1].iloc[i, 1]}\")\n",
        "\n",
        "print(f\"\\nConclusion: {best[0]} is the best feature for all metrics\")\n",
        "# => Max Power (best[0]) is the best feature for all metrics\n",
        "id_BestFeature_in_result = [i for i in range(len(result)) if result[i][0] == best[0]][0]\n",
        "\n",
        "beta0 = result[id_BestFeature_in_result][2]\n",
        "beta1 = result[id_BestFeature_in_result][3]\n",
        "F = result[id_BestFeature_in_result][0]\n",
        "print(f\"\\n{F} formula: Price = {beta0:.2f} + {beta1:.2f} * {F}\")\n",
        "\n",
        "print(\"\\nEvaluation metrics on Training Set:\")\n",
        "result[id_BestFeature_in_result][1]\n",
        "\n",
        "weight_simpleLR = [beta0, beta1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:16:03.818577Z",
          "start_time": "2025-04-02T09:16:03.805085Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "0Mj_X8COSWxq",
        "outputId": "09c28964-2da0-48a4-f54a-5da333aca423"
      },
      "outputs": [],
      "source": [
        "# Best model on training set:\n",
        "print(f'Best feature: {result[id_BestFeature_in_result][0]}')\n",
        "beta0 = result[id_BestFeature_in_result][2]\n",
        "beta1 = result[id_BestFeature_in_result][3]\n",
        "\n",
        "# Predict the price of a car based on the best feature (train set)\n",
        "y_pred = (beta0 + beta1 * X_train[result[id_BestFeature_in_result][0]]).round().astype(int)\n",
        "df = pd.DataFrame({'Actual': y_train, 'Predicted': y_pred})\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:16:04.881089Z",
          "start_time": "2025-04-02T09:16:04.852555Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "-Zz8Ay_xSWxr",
        "outputId": "7ee20924-21df-46b2-a5df-04bc212be722"
      },
      "outputs": [],
      "source": [
        "print(\"\\nEvaluation metrics on Validation Set:\")\n",
        "y_pred = beta0 + beta1 * X_val['Model']\n",
        "\n",
        "model_eval = evaluate_model(y_val, y_pred)\n",
        "model_eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:16:06.744425Z",
          "start_time": "2025-04-02T09:16:06.573491Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "s6mVQVGnSWxr",
        "outputId": "8a018962-c7cb-4adc-86a5-5e527fe27ae8"
      },
      "outputs": [],
      "source": [
        "# Predict the price of a car based on the best feature (train set)\n",
        "y_pred = (beta0 + beta1 * X_val[result[id_BestFeature_in_result][0]]).round().astype(int)\n",
        "df = pd.DataFrame({'Actual': y_val, 'Predicted': y_pred})\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:16:11.068549Z",
          "start_time": "2025-04-02T09:16:10.561058Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "YnCY6ICnSWxr",
        "outputId": "1c390c03-a865-45bb-d2b1-5b0e55a21114"
      },
      "outputs": [],
      "source": [
        "# Plot line of best fit\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Training set\n",
        "axes[0].scatter(X_train['Model'], y_train, color='blue', alpha=0.5, label='Actual Prices')\n",
        "sorted_idx_train = X_train['Model'].argsort()\n",
        "X_sorted_train = X_train['Model'].iloc[sorted_idx_train]\n",
        "y_line_train = beta0 + beta1 * X_sorted_train\n",
        "axes[0].plot(X_sorted_train, y_line_train, color='red', linewidth=2, label='Predicted Line')\n",
        "axes[0].set_xlabel(\"Model (Encoded)\")\n",
        "axes[0].set_ylabel(\"Price\")\n",
        "axes[0].set_title(\"Training Set: Actual vs Predicted\")\n",
        "axes[0].legend()\n",
        "\n",
        "# Validation set\n",
        "axes[1].scatter(X_val['Model'], y_val, color='green', alpha=0.5, label='Actual Prices')\n",
        "sorted_idx_val = X_val['Model'].argsort()\n",
        "X_sorted_val = X_val['Model'].iloc[sorted_idx_val]\n",
        "y_line_val = beta0 + beta1 * X_sorted_val\n",
        "axes[1].plot(X_sorted_val, y_line_val, color='orange', linewidth=2, label='Predicted Line')\n",
        "axes[1].set_xlabel(\"Model (Encoded)\")\n",
        "axes[1].set_ylabel(\"Price\")\n",
        "axes[1].set_title(\"Validation Set: Actual vs Predicted\")\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfNxCaAMSWxr"
      },
      "source": [
        "## Part VII: Multiple Linear Regression model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzETFzdrSWxr"
      },
      "source": [
        "### VII.1 Build Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:16:12.398792Z",
          "start_time": "2025-04-02T09:16:12.373405Z"
        },
        "id": "B8D4m6qESWxr"
      },
      "outputs": [],
      "source": [
        "class MultipleLinearRegression:\n",
        "    def __init__(self, X_train, y_train, X_val, y_val):\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.X_val = X_val\n",
        "        self.y_val = y_val\n",
        "        self.thetas = np.zeros(self.X_train.shape[1])\n",
        "        self.train_predictions = None\n",
        "        self.val_predictions = None\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.epochs = None\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions for given input data\"\"\"\n",
        "        return np.dot(X, self.thetas)\n",
        "\n",
        "    def compute_loss(self, predictions, y, loss_fn=None, alpha=0):\n",
        "        \"\"\"Calculate loss\"\"\"\n",
        "        mse = np.mean((y - predictions) ** 2)\n",
        "        if loss_fn is None:\n",
        "            return mse\n",
        "        else:\n",
        "            penalty = 0\n",
        "            if loss_fn == 'lasso':\n",
        "                penalty = np.sum(np.abs(self.thetas))\n",
        "            elif loss_fn == 'ridge':\n",
        "                penalty = np.sum(self.thetas ** 2)\n",
        "            loss = mse + alpha * penalty\n",
        "            return loss\n",
        "\n",
        "    def gradient(self, X, predictions, y):\n",
        "        \"\"\"Compute gradients for weight updates\"\"\"\n",
        "        return np.dot(X.T, (predictions - y)) / len(y)\n",
        "\n",
        "    def gradient_lasso(self, X, predictions, y, alpha):\n",
        "        \"\"\"Compute gradients for weight updates\"\"\"\n",
        "        n = len(y)\n",
        "        mse_gradient = -(2 / n) * np.dot(X.T, (y - predictions))\n",
        "        l1_gradient = alpha * np.sign(self.thetas)\n",
        "        gradient = mse_gradient + l1_gradient\n",
        "        return gradient\n",
        "\n",
        "    def gradient_ridge(self, X, predictions, y, alpha):\n",
        "        \"\"\"Compute gradients for weight updates\"\"\"\n",
        "        n = len(y)\n",
        "        mse_gradient = -(2 / n) * np.dot(X.T, (y - predictions))\n",
        "        l2_gradient = 2 * alpha * self.thetas\n",
        "        gradient = mse_gradient + l2_gradient\n",
        "        return gradient\n",
        "\n",
        "    def update_weights(self, learning_rate, gradient):\n",
        "        \"\"\"Update model parameters\"\"\"\n",
        "        return self.thetas - learning_rate * gradient\n",
        "\n",
        "    def train(self, epochs, learning_rate, log_interval, loss_fn=None, alpha=0, logging=False):\n",
        "        \"\"\"Train the model\"\"\"\n",
        "        self.epochs = epochs\n",
        "        for epoch in range(epochs):\n",
        "            self.train_predictions = self.predict(self.X_train)\n",
        "            self.val_predictions = self.predict(self.X_val)\n",
        "\n",
        "            if loss_fn == None:\n",
        "                train_loss = self.compute_loss(self.train_predictions, self.y_train)\n",
        "                val_loss = self.compute_loss(self.val_predictions, self.y_val)\n",
        "                grad = self.gradient(self.X_train, self.train_predictions, self.y_train)\n",
        "            elif loss_fn == 'lasso':\n",
        "                train_loss = self.compute_loss(self.train_predictions, self.y_train, loss_fn, alpha)\n",
        "                val_loss = self.compute_loss(self.val_predictions, self.y_val, loss_fn, alpha)\n",
        "                grad = self.gradient_lasso(self.X_train, self.train_predictions, self.y_train, alpha)\n",
        "            elif loss_fn == 'ridge':\n",
        "                train_loss = self.compute_loss(self.train_predictions, self.y_train, loss_fn, alpha)\n",
        "                val_loss = self.compute_loss(self.val_predictions, self.y_val, loss_fn, alpha)\n",
        "                grad = self.gradient_ridge(self.X_train, self.train_predictions, self.y_train, alpha)\n",
        "\n",
        "            self.train_losses.append(train_loss)\n",
        "            self.val_losses.append(val_loss)\n",
        "\n",
        "            self.thetas = self.update_weights(learning_rate, grad)\n",
        "\n",
        "            if epoch % log_interval == 0 and logging:\n",
        "                print(f\"Epoch: {epoch} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    def get_parameters(self):\n",
        "        \"\"\"Return current model parameters\"\"\"\n",
        "        return self.thetas\n",
        "\n",
        "    def plot_losses(self):\n",
        "        \"\"\"Plot losses over epochs\"\"\"\n",
        "        plt.figure(figsize=(12, 7), dpi=100)\n",
        "\n",
        "        epochs = range(len(self.train_losses))\n",
        "\n",
        "        plt.plot(epochs, self.train_losses,\n",
        "                label='Training Loss',\n",
        "                color='dodgerblue',\n",
        "                linewidth=2.5,\n",
        "                alpha=0.9)\n",
        "        plt.fill_between(epochs, self.train_losses,\n",
        "                        color='dodgerblue',\n",
        "                        alpha=0.1)\n",
        "\n",
        "        plt.plot(epochs, self.val_losses,\n",
        "                label='Validation Loss',\n",
        "                color='tomato',\n",
        "                linewidth=2.5,\n",
        "                alpha=0.9)\n",
        "        plt.fill_between(epochs, self.val_losses,\n",
        "                        color='tomato',\n",
        "                        alpha=0.1)\n",
        "\n",
        "        plt.xlabel('Epoch', fontsize=14, fontweight='bold', color='darkslategray')\n",
        "        plt.ylabel('Loss (Log Scale)', fontsize=14, fontweight='bold', color='darkslategray')\n",
        "        plt.title('Training and Validation Loss over Epochs',\n",
        "                fontsize=16, fontweight='bold', color='navy', pad=20)\n",
        "\n",
        "        plt.legend(fontsize=12, loc='upper right', frameon=True,\n",
        "                facecolor='white', edgecolor='black', framealpha=0.9)\n",
        "\n",
        "        plt.grid(True, linestyle='--', alpha=0.6, color='gray')\n",
        "\n",
        "        plt.yscale('log')\n",
        "\n",
        "        plt.xticks(fontsize=12, color='darkslategray')\n",
        "        plt.yticks(fontsize=12, color='darkslategray')\n",
        "\n",
        "        ax = plt.gca()\n",
        "        ax.set_facecolor('#f7f7f7')\n",
        "        for spine in ax.spines.values():\n",
        "            spine.set_edgecolor('gray')\n",
        "            spine.set_linewidth(0.8)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czuAkOkoSWxs"
      },
      "source": [
        "### VII.3 Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:16:14.867581Z",
          "start_time": "2025-04-02T09:16:14.850478Z"
        },
        "id": "01BSURqUSWxs"
      },
      "outputs": [],
      "source": [
        "# Add bias\n",
        "X_train_bias = np.hstack((np.ones(shape=(X_train.shape[0], 1)), X_train))\n",
        "X_val_bias = np.hstack((np.ones(shape=(X_val.shape[0], 1)), X_val))\n",
        "\n",
        "# Min-Max scaler\n",
        "train_min = np.min(y_train)\n",
        "train_max = np.max(y_train)\n",
        "\n",
        "val_min = np.min(y_val)\n",
        "val_max = np.max(y_val)\n",
        "\n",
        "y_train_minmax = (y_train - train_min) / (train_max - train_min)\n",
        "y_val_minmax = (y_val - val_min) / (val_max - val_min)\n",
        "\n",
        "# Standardization\n",
        "train_mean = np.mean(y_train)\n",
        "train_std = np.std(y_train)\n",
        "val_mean = np.mean(y_val)\n",
        "val_std = np.std(y_val)\n",
        "\n",
        "y_train_standardized = (y_train - train_mean) / train_std\n",
        "y_val_standardized = (y_val - val_mean) / val_std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:16:15.436674Z",
          "start_time": "2025-04-02T09:16:15.431587Z"
        },
        "id": "j1keFR_CSWxt"
      },
      "outputs": [],
      "source": [
        "# List to record results\n",
        "evaluations = []\n",
        "evaluations_label_normalized = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:16:15.847457Z",
          "start_time": "2025-04-02T09:16:15.843003Z"
        },
        "id": "UBsGFaozSWxt"
      },
      "outputs": [],
      "source": [
        "# Models dictionary\n",
        "models = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqKdHBWdSWxt"
      },
      "source": [
        "We will experimenting different methods to find out which one works out the best. Base model is using all features and no bias. Next models are built to be expected to outperform this one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2j4dlYbSWxt"
      },
      "source": [
        "### VII.4 Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9M4hCC15R6ri"
      },
      "source": [
        "#### Model architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJNPdzIsSWxt"
      },
      "source": [
        "##### Base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:16:16.452234Z",
          "start_time": "2025-04-02T09:16:16.449153Z"
        },
        "id": "1idBbJO7SWxt"
      },
      "outputs": [],
      "source": [
        "epochs = 5000\n",
        "learning_rate = 0.1\n",
        "log_interval = epochs / 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:16:26.399044Z",
          "start_time": "2025-04-02T09:16:16.847239Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-WV8vO-SWxt",
        "outputId": "69f4939c-1fa0-454a-f962-c12ac8df35e1"
      },
      "outputs": [],
      "source": [
        "multiLR = MultipleLinearRegression(X_train_bias, y_train, X_val_bias, y_val)\n",
        "multiLR.train(epochs, learning_rate, log_interval, logging=True)\n",
        "models['Base'] = multiLR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Q9kpBQXhMhyv",
        "outputId": "a5c94f43-e239-4983-9287-a4fffd120860"
      },
      "outputs": [],
      "source": [
        "multiLR.plot_losses()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:17:03.768418Z",
          "start_time": "2025-04-02T09:17:03.761166Z"
        },
        "id": "wVF3sCW9SWxu"
      },
      "outputs": [],
      "source": [
        "# Evaluate on multiple metrics\n",
        "y_pred = multiLR.predict(X_val_bias)\n",
        "evaluations.append(evaluate_model(y_val, y_pred).loc[:, 'Value'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpduQHOBSWxu"
      },
      "source": [
        "##### No bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2SadqhRSWxv"
      },
      "source": [
        "Usually not recommended since it will reduce model's complexity but we still experiment it for validation's sake."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:17:03.831553Z",
          "start_time": "2025-04-02T09:17:03.827133Z"
        },
        "id": "6wQf-0R8SWxv"
      },
      "outputs": [],
      "source": [
        "epochs = 5000\n",
        "learning_rate = 0.1\n",
        "log_interval = epochs / 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:17:05.687237Z",
          "start_time": "2025-04-02T09:17:03.889934Z"
        },
        "id": "zP1K--NgSWxv"
      },
      "outputs": [],
      "source": [
        "multiLR = MultipleLinearRegression(X_train, y_train, X_val, y_val)\n",
        "multiLR.train(epochs, learning_rate, log_interval)\n",
        "models['No bias'] = multiLR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "zEtuRbrHNj1W",
        "outputId": "227455db-3e4a-4c8d-aaf9-ba0bc78064e8"
      },
      "outputs": [],
      "source": [
        "multiLR.plot_losses()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:17:13.996057Z",
          "start_time": "2025-04-02T09:17:13.990577Z"
        },
        "id": "0wAAuoTMSWxw"
      },
      "outputs": [],
      "source": [
        "# Evaluate on multiple metrics\n",
        "y_pred = multiLR.predict(X_val)\n",
        "evaluations.append(evaluate_model(y_val, y_pred).loc[:, 'Value'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnOQNENmSWxw"
      },
      "source": [
        "#### Label Normalization\n",
        "So far, we have been using normalized features to predict the original labels. In this section, we will explore scaling the labels to a smaller range to facilitate more efficient training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXPH4IIQSWx0"
      },
      "source": [
        "##### Min-Max Scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:17:17.064121Z",
          "start_time": "2025-04-02T09:17:17.060365Z"
        },
        "id": "jIbhXMw4SWx0"
      },
      "outputs": [],
      "source": [
        "epochs = 100\n",
        "learning_rate = 0.1\n",
        "log_interval = epochs / 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:17:17.158720Z",
          "start_time": "2025-04-02T09:17:17.116076Z"
        },
        "id": "jJhjriLCSWx0"
      },
      "outputs": [],
      "source": [
        "multiLR = MultipleLinearRegression(X_train_bias, y_train_minmax, X_val_bias, y_val_minmax)\n",
        "multiLR.train(epochs, learning_rate, log_interval)\n",
        "models['Min-Max Scaler'] = multiLR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:17:18.376986Z",
          "start_time": "2025-04-02T09:17:18.372764Z"
        },
        "id": "IRXDIvGSSWx1"
      },
      "outputs": [],
      "source": [
        "# Evaluate on multiple metrics\n",
        "y_pred = multiLR.predict(X_val_bias)\n",
        "evaluations_label_normalized.append(evaluate_model(y_val_minmax, y_pred).loc[:, 'Value'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:17:18.475171Z",
          "start_time": "2025-04-02T09:17:18.470447Z"
        },
        "id": "qHxRz2aoSWx1"
      },
      "outputs": [],
      "source": [
        "# Retransform to initial range\n",
        "y_pred_original = y_pred * (train_max - train_min) + train_min\n",
        "evaluations.append(evaluate_model(y_val, y_pred_original).loc[:, 'Value'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsHRSz22SWx1"
      },
      "source": [
        "##### Standardization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:17:18.488010Z",
          "start_time": "2025-04-02T09:17:18.484178Z"
        },
        "id": "Cmh_WjLPSWx1"
      },
      "outputs": [],
      "source": [
        "epochs = 2000\n",
        "learning_rate = 0.1\n",
        "log_interval = epochs / 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:17:19.222157Z",
          "start_time": "2025-04-02T09:17:18.543708Z"
        },
        "id": "Cj5swnk6SWx2"
      },
      "outputs": [],
      "source": [
        "multiLR = MultipleLinearRegression(X_train_bias, y_train_standardized, X_val_bias, y_val_standardized)\n",
        "multiLR.train(epochs, learning_rate, log_interval)\n",
        "models['Standardization'] = multiLR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:17:25.481434Z",
          "start_time": "2025-04-02T09:17:25.476649Z"
        },
        "id": "TBVCxhv5SWx3"
      },
      "outputs": [],
      "source": [
        "# Evaluate on multiple metrics\n",
        "y_pred = multiLR.predict(X_val_bias)\n",
        "evaluations_label_normalized.append(evaluate_model(y_val_standardized, y_pred).loc[:, 'Value'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:17:25.580511Z",
          "start_time": "2025-04-02T09:17:25.576268Z"
        },
        "id": "I-UFLeETSWx3"
      },
      "outputs": [],
      "source": [
        "# Retransform to initial range\n",
        "y_pred_original = y_pred * val_std + val_mean\n",
        "evaluations.append(evaluate_model(y_val, y_pred_original).loc[:, 'Value'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzLssHMbSQx_"
      },
      "source": [
        "#### Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcJPXafZTZxf"
      },
      "source": [
        "##### Lasso Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDJopC3pErHA"
      },
      "outputs": [],
      "source": [
        "epochs = 3000\n",
        "log_interval = epochs / 10\n",
        "loss_fn = 'lasso'\n",
        "learning_rate = 0.1\n",
        "alpha = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:17:26.662382Z",
          "start_time": "2025-04-02T09:17:25.642960Z"
        },
        "id": "AkDzf9xDXfFj"
      },
      "outputs": [],
      "source": [
        "multiLR = MultipleLinearRegression(X_train_bias, y_train, X_val_bias, y_val)\n",
        "multiLR.train(epochs, learning_rate, log_interval, loss_fn, alpha=0.1)\n",
        "models['Lasso'] = multiLR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 637
        },
        "id": "QyXLUmVzCDXW",
        "outputId": "1cfed18f-41cd-42ae-ec8c-504e9b2bafaa"
      },
      "outputs": [],
      "source": [
        "multiLR.plot_losses()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "HLIITradB2IR",
        "outputId": "da0a9ce5-acce-4626-fce9-dc42559056b2"
      },
      "outputs": [],
      "source": [
        "y_pred = multiLR.predict(X_val_bias)\n",
        "evaluate_model(y_val, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:17:31.586595Z",
          "start_time": "2025-04-02T09:17:31.581823Z"
        },
        "id": "woXDerjEZLxD"
      },
      "outputs": [],
      "source": [
        "# Evaluate on multiple metrics\n",
        "y_pred = multiLR.predict(X_val_bias)\n",
        "evaluations.append(evaluate_model(y_val, y_pred).loc[:, 'Value'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJj8Gei-vfjg"
      },
      "source": [
        "##### Ridge Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:17:31.639281Z",
          "start_time": "2025-04-02T09:17:31.635767Z"
        },
        "id": "_fT6IkZmvfjh"
      },
      "outputs": [],
      "source": [
        "epochs = 3000\n",
        "log_interval = epochs / 10\n",
        "loss_fn = 'ridge'\n",
        "learning_rate = 0.1\n",
        "alpha = 0.0001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:17:32.758271Z",
          "start_time": "2025-04-02T09:17:31.742593Z"
        },
        "id": "ymZdesFFvfjh"
      },
      "outputs": [],
      "source": [
        "multiLR = MultipleLinearRegression(X_train_bias, y_train, X_val_bias, y_val)\n",
        "multiLR.train(epochs, learning_rate, log_interval, loss_fn, alpha)\n",
        "models['Ridge'] = multiLR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 637
        },
        "id": "n9c2d3cxFr69",
        "outputId": "2d81a590-7194-4c9e-f841-7325e841a10d"
      },
      "outputs": [],
      "source": [
        "multiLR.plot_losses()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:17:37.504579Z",
          "start_time": "2025-04-02T09:17:37.499796Z"
        },
        "id": "GDiXFCQMvfji"
      },
      "outputs": [],
      "source": [
        "# Evaluate on multiple metrics\n",
        "y_pred = multiLR.predict(X_val_bias)\n",
        "evaluations.append(evaluate_model(y_val, y_pred).loc[:, 'Value'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYEqRqnwSWx3"
      },
      "source": [
        "### VII.5 Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:17:37.560582Z",
          "start_time": "2025-04-02T09:17:37.549763Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "p3rmbfbASWx4",
        "outputId": "f5f20b85-c3fc-4e44-fce1-4477c4d8b5da"
      },
      "outputs": [],
      "source": [
        "# Evaluation summary\n",
        "summary = pd.concat(evaluations, axis=1)\n",
        "summary.index = ['Mean Squared Error', 'Root Mean Squared Error', 'Mean Absolute Error', 'R^2 Score']\n",
        "summary.columns = ['Base', 'No bias', 'Min-Max scaler', 'Standardization', 'Lasso', 'Ridge']\n",
        "summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:17:37.646645Z",
          "start_time": "2025-04-02T09:17:37.638760Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "jnA8IJJWSWx4",
        "outputId": "a6daeef7-35b9-4be6-f459-5dfbc8c4505e"
      },
      "outputs": [],
      "source": [
        "# Evaluation summary on normalized labels\n",
        "summary_label_normalized = pd.concat(evaluations_label_normalized, axis=1)\n",
        "summary_label_normalized.index = ['Mean Squared Error', 'Root Mean Squared Error', 'Mean Absolute Error', 'R^2 Score']\n",
        "summary_label_normalized.columns = ['Min-Max scaler', 'Standardization']\n",
        "summary_label_normalized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3wqPZ9qSWx4"
      },
      "source": [
        "After experimenting with various approaches, the model with bias achieved the highest R² score. The elevated loss values can be attributed to the significant disparity in the range between the features and labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:17:37.711932Z",
          "start_time": "2025-04-02T09:17:37.704668Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "6NrVeGvZSWx4",
        "outputId": "c477022a-c8a6-4095-9395-4b70b50df6cf"
      },
      "outputs": [],
      "source": [
        "data.data['Price'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnLiBvT0SWx5"
      },
      "source": [
        "However, the high R² score indicates that the model can account for 96.7% of the variance in the data, demonstrating that the base model, the model with bias, and the standardization method are all performing effectively. Ultimately, the model with bias was chosen as the final result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:17:38.099123Z",
          "start_time": "2025-04-02T09:17:38.090806Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "UMVlHlrcSWx5",
        "outputId": "985ed321-d938-4d84-f80d-e9da005e1f13"
      },
      "outputs": [],
      "source": [
        "model_multi = models['No bias']\n",
        "y_pred = model_multi.predict(X_val)\n",
        "result = pd.DataFrame({'Actual': y_val, 'Predicted': y_pred})\n",
        "result.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:17:38.346592Z",
          "start_time": "2025-04-02T09:17:38.340459Z"
        },
        "id": "AfuU2o4oymSa"
      },
      "outputs": [],
      "source": [
        "def plot_true_vs_predicted_train_val(train_true, train_pred, val_true, val_pred):\n",
        "    train_true = np.array(train_true)\n",
        "    train_pred = np.array(train_pred)\n",
        "    val_true = np.array(val_true)\n",
        "    val_pred = np.array(val_pred)\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "    # --- Training Set ---\n",
        "    # Scatter Plot: True vs Predicted (Training Set)\n",
        "    ax1.scatter(train_pred, train_true, color='blue', alpha=0.5, label='Data Points')\n",
        "\n",
        "    # Vẽ đường y = x (đường lý tưởng nếu dự đoán hoàn hảo)\n",
        "    min_val_train = min(min(train_true), min(train_pred))\n",
        "    max_val_train = max(max(train_true), max(train_pred))\n",
        "    ax1.plot([min_val_train, max_val_train], [min_val_train, max_val_train],\n",
        "             color='red', linestyle='--', label='Perfect Prediction Line')\n",
        "\n",
        "    ax1.set_xlabel('Predicted Labels')\n",
        "    ax1.set_ylabel('True Labels')\n",
        "    ax1.set_title('Training Set: True vs Predicted')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # --- Validation Set ---\n",
        "    # Scatter Plot: True vs Predicted (Validation Set)\n",
        "    ax2.scatter(val_pred, val_true, color='blue', alpha=0.5, label='Data Points')\n",
        "\n",
        "    # Vẽ đường y = x (đường lý tưởng nếu dự đoán hoàn hảo)\n",
        "    min_val_val = min(min(val_true), min(val_pred))\n",
        "    max_val_val = max(max(val_true), max(val_pred))\n",
        "    ax2.plot([min_val_val, max_val_val], [min_val_val, max_val_val],\n",
        "             color='red', linestyle='--', label='Perfect Prediction Line')\n",
        "\n",
        "    ax2.set_xlabel('Predicted Labels')\n",
        "    ax2.set_ylabel('True Labels')\n",
        "    ax2.set_title('Validation Set: True vs Predicted')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:17:39.103641Z",
          "start_time": "2025-04-02T09:17:38.754733Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "CvgvqeipyoNJ",
        "outputId": "c4de0fe9-7f75-4d87-8630-dbe92c0f04cf"
      },
      "outputs": [],
      "source": [
        "y_pred_train = model_multi.predict(X_train)\n",
        "y_pred_val = model_multi.predict(X_val)\n",
        "\n",
        "plot_true_vs_predicted_train_val(y_train, y_pred_train, y_val, y_pred_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsCG0a_6SWx5"
      },
      "source": [
        "## Part VIII: Polynomial Regression model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZzU6fx8SWx5"
      },
      "source": [
        "### VI.1: Model building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T16:25:38.787007Z",
          "start_time": "2025-04-02T16:25:38.680443Z"
        },
        "id": "_57tCC__SWx5"
      },
      "outputs": [],
      "source": [
        "from itertools import combinations_with_replacement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T16:25:39.444139Z",
          "start_time": "2025-04-02T16:25:39.405510Z"
        },
        "id": "TW36UxBXSWx6"
      },
      "outputs": [],
      "source": [
        "class PolynomialFeatures:\n",
        "    \"\"\"\n",
        "    Tạo ma trận đặc trưng đa thức.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    degree : int, mặc định=2\n",
        "        Bậc của đa thức.\n",
        "\n",
        "    include_bias : bool, mặc định=True\n",
        "        Nếu True, thêm cột toàn 1 vào ma trận (hằng số).\n",
        "\n",
        "    interaction_only : bool, mặc định=False\n",
        "        Nếu True, chỉ bao gồm các tương tác giữa các đặc trưng.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, degree=2, include_bias=True, interaction_only=False):\n",
        "        self.degree = degree\n",
        "        self.include_bias = include_bias\n",
        "        self.interaction_only = interaction_only\n",
        "\n",
        "    def fit(self, X):\n",
        "        \"\"\"\n",
        "        Tính số lượng đặc trưng đầu ra.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like, shape (n_samples, n_features)\n",
        "            Ma trận đặc trưng đầu vào.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "        \"\"\"\n",
        "        n_samples, n_features = np.asarray(X).shape\n",
        "        self.n_input_features_ = n_features\n",
        "\n",
        "        combinations = []\n",
        "        for d in range(0, self.degree + 1):\n",
        "            if d == 0 and not self.include_bias:\n",
        "                continue\n",
        "            if d == 1:\n",
        "                combinations.extend(range(n_features))\n",
        "                continue\n",
        "\n",
        "            if self.interaction_only:\n",
        "                combinations.extend(combinations_with_replacement(range(n_features), d))\n",
        "            else:\n",
        "                combinations.extend([c for c in combinations_with_replacement(range(n_features), d)\n",
        "                                    if len(set(c)) == 1])\n",
        "\n",
        "        self.n_output_features_ = len(combinations) + (1 if self.include_bias else 0)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Chuyển đổi đặc trưng thành đặc trưng đa thức.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like, shape (n_samples, n_features)\n",
        "            Ma trận đặc trưng đầu vào.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        XP : np.ndarray, shape (n_samples, n_output_features)\n",
        "            Ma trận đặc trưng đa thức.\n",
        "        \"\"\"\n",
        "        X = np.asarray(X)\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        if n_features != self.n_input_features_:\n",
        "            raise ValueError(\"X shape does not match training shape\")\n",
        "\n",
        "        # Khởi tạo ma trận đầu ra\n",
        "        XP = np.ones((n_samples, 0))\n",
        "\n",
        "        # Thêm hằng số nếu include_bias=True\n",
        "        if self.include_bias:\n",
        "            XP = np.hstack((np.ones((n_samples, 1)), XP))\n",
        "\n",
        "        # Thêm đặc trưng ban đầu (bậc 1)\n",
        "        if self.degree >= 1:\n",
        "            XP = np.hstack((XP, X))\n",
        "\n",
        "        # Tạo đặc trưng đa thức bậc cao hơn\n",
        "        for d in range(2, self.degree + 1):\n",
        "            if self.interaction_only:\n",
        "                combs = [c for c in combinations_with_replacement(range(n_features), d)\n",
        "                         if len(set(c)) > 1]\n",
        "            else:\n",
        "                combs = list(combinations_with_replacement(range(n_features), d))\n",
        "\n",
        "            for comb in combs:\n",
        "                new_col = np.ones((n_samples, 1))\n",
        "                for i in comb:\n",
        "                    new_col = new_col * X[:, i:i+1]\n",
        "                XP = np.hstack((XP, new_col))\n",
        "\n",
        "        return XP\n",
        "\n",
        "    def fit_transform(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Fit và transform cùng một lúc.\n",
        "        \"\"\"\n",
        "        return self.fit(X).transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T16:25:40.188460Z",
          "start_time": "2025-04-02T16:25:40.173321Z"
        },
        "id": "jsuJWgEiSWx6"
      },
      "outputs": [],
      "source": [
        "class MultipleRegression:\n",
        "    \"\"\"\n",
        "    Hồi quy đa thức bằng OLS (Ordinary Least Squares).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    fit_intercept : bool, mặc định=True\n",
        "        Có tính hằng số hay không.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, fit_intercept=True):\n",
        "        self.fit_intercept = fit_intercept\n",
        "        self.coef_ = None\n",
        "        self.intercept_ = None\n",
        "\n",
        "    def fit(self, X, y, alpha=0.1):\n",
        "        \"\"\"\n",
        "        Huấn luyện mô hình hồi quy đa thức bằng OLS (Ordinary Least Squares)\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like, shape (n_samples, n_features)\n",
        "            Ma trận đặc trưng.\n",
        "\n",
        "        y : array-like, shape (n_samples,)\n",
        "            Vector mục tiêu.\n",
        "\n",
        "        alpha : float, mặc định=0.1\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "        \"\"\"\n",
        "        n_features = X.shape[1]\n",
        "\n",
        "        # Thêm chính quy hóa L2 (Ridge): (X^T X + αI)^(-1) X^T y\n",
        "        identity = np.identity(n_features)\n",
        "        beta = np.linalg.inv(X.T.dot(X) + alpha * identity).dot(X.T).dot(y)\n",
        "\n",
        "        self.weight = beta\n",
        "        if self.fit_intercept:\n",
        "            self.intercept_ = beta[0]\n",
        "            self.coef_ = beta[1:]\n",
        "        else:\n",
        "            self.intercept_ = 0.0\n",
        "            self.coef_ = beta\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Dự đoán sử dụng mô hình đã huấn luyện.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like, shape (n_samples, n_features)\n",
        "            Ma trận đặc trưng.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y_pred : array, shape (n_samples,)\n",
        "            Giá trị dự đoán.\n",
        "        \"\"\"\n",
        "        X = np.asarray(X)\n",
        "        return X.dot(self.weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T16:33:46.169901Z",
          "start_time": "2025-04-02T16:33:46.143700Z"
        },
        "id": "haA0WTDFSWx6"
      },
      "outputs": [],
      "source": [
        "class PolynomialRegression:\n",
        "    \"\"\"\n",
        "    Hồi quy đa thức bằng phương pháp OLS.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    degree : int, mặc định=2\n",
        "        Bậc của đa thức.\n",
        "\n",
        "    include_bias : bool, mặc định=True\n",
        "        Có thêm cột toàn 1 vào ma trận không.\n",
        "\n",
        "    interaction_only : bool, mặc định=False\n",
        "        Chỉ bao gồm các tương tác giữa các đặc trưng.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, degree=2, include_bias=True, interaction_only=False):\n",
        "        self.degree = degree\n",
        "        self.include_bias = include_bias\n",
        "        self.interaction_only = interaction_only\n",
        "        self.poly = PolynomialFeatures(degree=degree,\n",
        "                                       include_bias=include_bias,\n",
        "                                       interaction_only=interaction_only)\n",
        "        self.multiple_regression = MultipleRegression(fit_intercept=False if include_bias else True)\n",
        "        self.train_losses = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Huấn luyện mô hình hồi quy đa thức.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like, shape (n_samples, n_features)\n",
        "            Ma trận đặc trưng.\n",
        "\n",
        "        y : array-like, shape (n_samples,)\n",
        "            Vector mục tiêu.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "        \"\"\"\n",
        "        X_poly = self.poly.fit_transform(X)\n",
        "        self.multiple_regression.fit(X_poly, y)\n",
        "        self.weights = self.multiple_regression.weight\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Dự đoán giá trị khi sử dụng mô hình đã huấn luyện.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like, shape (n_samples, n_features)\n",
        "            Ma trận đặc trưng.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y_pred : array, shape (n_samples,)\n",
        "            Giá trị dự đoán.\n",
        "        \"\"\"\n",
        "        X_poly = self.poly.transform(X)\n",
        "        return self.multiple_regression.predict(X_poly)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T16:25:41.848254Z",
          "start_time": "2025-04-02T16:25:41.821857Z"
        },
        "id": "dBEdzLwu527I"
      },
      "outputs": [],
      "source": [
        "class PolynomialRegressionGD:\n",
        "    \"\"\"\n",
        "    Hồi quy đa thức Polynomial Regression bằng phương pháp Gradient Descent.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, degree=2, learning_rate=0.01, n_iterations=1000, batch_size=None,\n",
        "                 random_state=None, epsilon=1e-8, include_bias=True, interaction_only=False):\n",
        "        \"\"\"\n",
        "        Khởi tạo mô hình Polynomial Regression.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        degree : int, default=2\n",
        "            Bậc của đa thức\n",
        "        learning_rate : float, default=0.01\n",
        "            Tốc độ học (alpha)\n",
        "        n_iterations : int, default=1000\n",
        "            Số lần lặp tối đa (epochs)\n",
        "        batch_size : int, default=None\n",
        "            Kích thước batch (None = dùng toàn bộ dữ liệu)\n",
        "        random_state : int, default=None\n",
        "            Giá trị khởi tạo cho random\n",
        "        epsilon : float, default=1e-8\n",
        "            Ngưỡng hội tụ\n",
        "        \"\"\"\n",
        "        self.degree = degree\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.batch_size = batch_size\n",
        "        self.random_state = random_state\n",
        "        self.epsilon = epsilon\n",
        "        self.lambda_reg = 0.01\n",
        "        self.weights = None\n",
        "        self.history = {'loss': [], 'weights': []}\n",
        "        self.include_bias = include_bias\n",
        "        self.interaction_only = interaction_only\n",
        "        self.poly = PolynomialFeatures(degree=degree,\n",
        "                                       include_bias=include_bias,\n",
        "                                       interaction_only=interaction_only)\n",
        "\n",
        "        if random_state is not None:\n",
        "            np.random.seed(random_state)\n",
        "\n",
        "    def _compute_cost(self, X, y, weights):\n",
        "        \"\"\"\n",
        "        Tính toán hàm mất mát (MSE - Mean Squared Error).\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            Ma trận đặc trưng\n",
        "        y : ndarray, shape (n_samples,)\n",
        "            Mảng giá trị thực tế\n",
        "        weights : ndarray, shape (n_features,)\n",
        "            Mảng trọng số của mô hình\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        float : Giá trị hàm mất mát\n",
        "        \"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "        y_pred = self._predict_with_weights(X, weights)\n",
        "        error = y_pred - y\n",
        "        cost = np.sum(error ** 2) / (2 * n_samples)\n",
        "\n",
        "        # Thêm chính quy hóa L2\n",
        "        reg_term = (self.lambda_reg / (2*n_samples)) * np.sum(weights[1:]**2)\n",
        "        cost = cost + reg_term\n",
        "        return cost\n",
        "\n",
        "    def _compute_gradient(self, X, y, weights):\n",
        "        \"\"\"\n",
        "        Tính toán gradient của hàm mất mát.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            Ma trận đặc trưng\n",
        "        y : ndarray, shape (n_samples,)\n",
        "            Mảng giá trị thực tế\n",
        "        weights : ndarray, shape (n_features,)\n",
        "            Mảng trọng số của mô hình\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        ndarray, shape (n_features,) : Gradient của hàm mất mát\n",
        "        \"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "        y_pred = self._predict_with_weights(X, weights)\n",
        "        error = y_pred - y\n",
        "        gradient = (X.T @ error) / n_samples\n",
        "\n",
        "        # Thêm chính quy hóa L2 (không áp dụng cho bias)\n",
        "        reg_gradient = np.zeros_like(weights)\n",
        "        if self.include_bias:\n",
        "            reg_gradient[1:] = (self.lambda_reg / n_samples) * weights[1:]\n",
        "        else:\n",
        "            reg_gradient = (self.lambda_reg / n_samples) * weights\n",
        "\n",
        "        gradient = gradient + reg_gradient\n",
        "        return gradient\n",
        "\n",
        "    def _predict_with_weights(self, X, weights):\n",
        "        \"\"\"\n",
        "        Dự đoán giá trị với trọng số cho trước.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : ndarray\n",
        "            Ma trận đặc trưng\n",
        "        weights : ndarray\n",
        "            Mảng trọng số\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        ndarray : Mảng giá trị dự đoán\n",
        "        \"\"\"\n",
        "        return X @ weights\n",
        "\n",
        "    def fit(self, X, y, verbose=False):\n",
        "        \"\"\"\n",
        "        Huấn luyện mô hình trên dữ liệu X, y.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : array-like, shape (n_samples, n_features)\n",
        "            Mảng đặc trưng đầu vào\n",
        "        y : array-like, shape (n_samples,)\n",
        "            Mảng giá trị đầu ra\n",
        "        verbose : bool, default=False\n",
        "            Nếu True, in ra thông tin trong quá trình huấn luyện\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        self : Đối tượng\n",
        "        \"\"\"\n",
        "        # Chuyển đổi dữ liệu thành mảng numpy\n",
        "        X = np.asarray(X)\n",
        "        y = np.asarray(y)\n",
        "\n",
        "        # Tạo đặc trưng đa thức\n",
        "        X_poly = self.poly.fit_transform(X)\n",
        "        n_samples, n_features = X_poly.shape\n",
        "\n",
        "        # Khởi tạo trọng số\n",
        "        self.weights = np.zeros(n_features)\n",
        "\n",
        "        # Xác định kích thước batch\n",
        "        if self.batch_size is None or self.batch_size > n_samples:\n",
        "            self.batch_size = n_samples\n",
        "\n",
        "        # Quá trình huấn luyện\n",
        "        for iteration in range(self.n_iterations):\n",
        "            # Xáo trộn dữ liệu\n",
        "            indices = np.random.permutation(n_samples)\n",
        "            X_shuffled = X_poly[indices]\n",
        "            y_shuffled = y[indices]\n",
        "\n",
        "            # Mini-batch gradient descent\n",
        "            for i in range(0, n_samples, self.batch_size):\n",
        "                X_batch = X_shuffled[i:i + self.batch_size]\n",
        "                y_batch = y_shuffled[i:i + self.batch_size]\n",
        "\n",
        "                # Tính gradient và cập nhật trọng số\n",
        "                gradient = self._compute_gradient(X_batch, y_batch, self.weights)\n",
        "                self.weights -= self.learning_rate * gradient\n",
        "\n",
        "            # Tính và lưu trữ giá trị loss\n",
        "            current_loss = self._compute_cost(X_poly, y, self.weights)\n",
        "            self.history['loss'].append(current_loss)\n",
        "            self.history['weights'].append(self.weights.copy())\n",
        "\n",
        "            # In thông tin nếu verbose=True\n",
        "            if verbose and (iteration + 1) % max(1, self.n_iterations // 10) == 0:\n",
        "                print(f\"Epoch {iteration + 1}/{self.n_iterations}, Loss: {current_loss:.6f}\")\n",
        "\n",
        "            # Điều kiện dừng sớm\n",
        "            if iteration > 0 and abs(self.history['loss'][iteration-1] - current_loss) < self.epsilon:\n",
        "                if verbose:\n",
        "                    print(f\"Đã hội tụ sau {iteration + 1} epochs với loss: {current_loss:.6f}\")\n",
        "                break\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Dự đoán giá trị cho dữ liệu mới.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : array-like, shape (n_samples,) hoặc (n_samples, 1)\n",
        "            Mảng đặc trưng đầu vào\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        ndarray, shape (n_samples,) : Mảng giá trị dự đoán\n",
        "        \"\"\"\n",
        "        if self.weights is None:\n",
        "            raise ValueError(\"Mô hình chưa được huấn luyện.\")\n",
        "\n",
        "        X = np.array(X)\n",
        "        X_poly = self.poly.transform(X)\n",
        "        return self._predict_with_weights(X_poly, self.weights)\n",
        "\n",
        "    def plot_learning_curve(self):\n",
        "        \"\"\"\n",
        "        Vẽ đồ thị giá trị loss theo các epoch.\n",
        "        \"\"\"\n",
        "        if not self.history['loss']:\n",
        "            raise ValueError(\"Mô hình chưa được huấn luyện.\")\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(range(1, len(self.history['loss']) + 1), self.history['loss'], marker='o', linestyle='-', markersize=2)\n",
        "        plt.title('Đồ thị hàm mất mát theo epoch')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss (MSE)')\n",
        "        plt.grid(True)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T16:35:13.217638Z",
          "start_time": "2025-04-02T16:35:13.163401Z"
        },
        "id": "s9m8gCYt527J"
      },
      "outputs": [],
      "source": [
        "def plot_regression_line(model, X, y_true, name_plot=\"\"):\n",
        "    \"\"\"\n",
        "    Vẽ đồ thị đường hồi quy và dữ liệu.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    X : array-like, shape (n_samples,)\n",
        "        Mảng đặc trưng đầu vào\n",
        "    y : array-like, shape (n_samples,)\n",
        "        Mảng giá trị thực tế\n",
        "    name_plot : str, default=\"\"\n",
        "        Nhãn tiền tố của bảng đồ thị\n",
        "    \"\"\"\n",
        "    if model.weights is None:\n",
        "        raise ValueError(\"Mô hình chưa được huấn luyện.\")\n",
        "\n",
        "    y_pred = model.predict(X)\n",
        "    plt.figure(figsize=(15, 6))\n",
        "\n",
        "    # Plot 1: Scatter plot of predicted vs actual\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.scatter(y_true, y_pred, alpha=0.5, label='Actual Prices')\n",
        "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], label='Predicted Line', color='red')\n",
        "    plt.xlabel('Actual Values')\n",
        "    plt.ylabel('Price')\n",
        "    plt.title(name_plot + 'Actual vs Predicted')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot 2: Same scatter plot with log scale (helps with extreme values)\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.scatter(y_true, y_pred, alpha=0.5, label='Actual Prices')\n",
        "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], label='Predicted Line', color='red')\n",
        "    plt.xscale('log')\n",
        "    plt.yscale('log')\n",
        "    plt.xlabel('Actual Values (log scale)')\n",
        "    plt.ylabel('Price (log scale)')\n",
        "    plt.title(name_plot + 'Actual vs Predicted - Log Scale')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot 3: Histogram of errors\n",
        "    plt.subplot(1, 3, 3)\n",
        "    residuals = y_pred - y_true\n",
        "    plt.hist(residuals, bins=30, alpha=0.7, color='green')\n",
        "    plt.axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
        "    plt.xlabel('Residuals')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title(name_plot + 'Residuals Distribution')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def display_equation(model, feature_names=None, precision=4, top_k=None):\n",
        "    \"\"\"\n",
        "    Displays the polynomial regression equation based on the fitted weights.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    feature_names : list, optional\n",
        "        List of original feature names. If None, uses X1, X2, etc.\n",
        "    precision : int, default=4\n",
        "        Number of decimal places to display for coefficients\n",
        "    top_k : int, optional\n",
        "        If provided, only shows the top k coefficients by absolute value\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    str : The formatted equation string\n",
        "    \"\"\"\n",
        "    if model.weights is None:\n",
        "        raise ValueError(\"Model has not been trained yet.\")\n",
        "\n",
        "    if feature_names is None:\n",
        "        if hasattr(model.poly, 'n_input_features_'):\n",
        "            feature_names = [f\"X{i+1}\" for i in range(model.poly.n_input_features_)]\n",
        "        else:\n",
        "            feature_names = [f\"X{i+1}\" for i in range(1)]  # Default case\n",
        "\n",
        "    # Get all terms from polynomial transformation\n",
        "    powers = []\n",
        "    for i in range(1, model.degree + 1):\n",
        "        if i == 1:\n",
        "            # Linear terms\n",
        "            powers.extend([(j, 1) for j in range(len(feature_names))])\n",
        "        else:\n",
        "            # Higher degree terms including interactions\n",
        "            for combo in combinations_with_replacement(range(len(feature_names)), i):\n",
        "                term_powers = [0] * len(feature_names)\n",
        "                for idx in combo:\n",
        "                    term_powers[idx] += 1\n",
        "                powers.append((None, term_powers))\n",
        "\n",
        "    # Add bias term if included\n",
        "    if model.include_bias:\n",
        "        powers.insert(0, (None, None))\n",
        "\n",
        "    # Format the equation\n",
        "    terms = []\n",
        "\n",
        "    # Get coefficient data for potential top_k filtering\n",
        "    coef_data = []\n",
        "    for i, coef in enumerate(model.weights):\n",
        "        if i < len(powers):\n",
        "            coef_data.append((i, coef, powers[i]))\n",
        "\n",
        "    # Filter by top_k if specified\n",
        "    if top_k is not None and top_k < len(coef_data):\n",
        "        coef_data = sorted(coef_data, key=lambda x: abs(x[1]), reverse=True)[:top_k]\n",
        "        print(f\"Showing top {top_k} terms by coefficient magnitude:\")\n",
        "\n",
        "    # Format each term\n",
        "    for i, coef, power_info in coef_data:\n",
        "        if abs(coef) < 1e-10:  # Skip effectively zero coefficients\n",
        "            continue\n",
        "\n",
        "        if i == 0 and model.include_bias:\n",
        "            # Bias term\n",
        "            terms.append(f\"{coef:.{precision}f}\")\n",
        "        else:\n",
        "            # Format term based on power information\n",
        "            if power_info[0] is not None:\n",
        "                # Simple linear term\n",
        "                term = feature_names[power_info[0]]\n",
        "            else:\n",
        "                # Higher-degree term\n",
        "                if power_info[1] is None:\n",
        "                    continue  # Skip if power info is not valid\n",
        "\n",
        "                term_parts = []\n",
        "                for j, power in enumerate(power_info[1]):\n",
        "                    if power > 0:\n",
        "                        if power == 1:\n",
        "                            term_parts.append(f\"{feature_names[j]}\")\n",
        "                        else:\n",
        "                            term_parts.append(f\"{feature_names[j]}^{power}\")\n",
        "\n",
        "                term = \" × \".join(term_parts)\n",
        "\n",
        "            # Add coefficient with sign\n",
        "            if coef >= 0:\n",
        "                if terms:  # Not the first term\n",
        "                    terms.append(f\"+ {coef:.{precision}f} × {term}\")\n",
        "                else:  # First term\n",
        "                    terms.append(f\"{coef:.{precision}f} × {term}\")\n",
        "            else:\n",
        "                terms.append(f\"- {abs(coef):.{precision}f} × {term}\")\n",
        "\n",
        "    equation = \" \".join(terms)\n",
        "\n",
        "    print(f\"\\nPolynomial Regression Equation (Degree {model.degree}):\")\n",
        "    print(f\"y = {equation}\")\n",
        "    print(f\"\\nTotal number of coefficients: {len(model.weights)}\")\n",
        "\n",
        "    return equation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smA4Mnyy527J"
      },
      "source": [
        "### VI.2: Training and evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTeVZmuU527K"
      },
      "source": [
        "#### a) Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T17:08:47.804874Z",
          "start_time": "2025-04-02T17:08:47.716871Z"
        },
        "id": "_HuJdE4u527K"
      },
      "outputs": [],
      "source": [
        "X_train = train_data.drop(columns=['Price'])\n",
        "y_train = train_data['Price']\n",
        "\n",
        "X_val = validation_data.drop(columns=['Price'])\n",
        "y_val = validation_data['Price']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJ_Yg_DI527K"
      },
      "source": [
        "#### b) Train by PolynomialRegression (using OLS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T18:40:18.596737Z",
          "start_time": "2025-04-02T18:40:18.307275Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JDEj6wdSWx7",
        "outputId": "6b51bf0b-920c-4e53-fc07-c12e25391c9a"
      },
      "outputs": [],
      "source": [
        "modelPoly = PolynomialRegression(degree=3)\n",
        "modelPoly.fit(X_train, y_train)\n",
        "print(\"Model trained successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T17:09:11.863600Z",
          "start_time": "2025-04-02T17:09:01.531255Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "BfdRb3uQ527P",
        "outputId": "37bbec2e-7814-478c-eb0d-4acf7fb42fe3"
      },
      "outputs": [],
      "source": [
        "# On Traning Set\n",
        "y_pred = modelPoly.predict(X_train)\n",
        "model_eval = evaluate_model(y_train, y_pred)\n",
        "print('Evaluate metrics on Training Set:')\n",
        "model_eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T17:09:18.700938Z",
          "start_time": "2025-04-02T17:09:14.794679Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "VsPO9Yl8527Q",
        "outputId": "df4af458-0d2d-4ac0-aec3-60dea3e50af2"
      },
      "outputs": [],
      "source": [
        "# On Validation Set\n",
        "y_pred = modelPoly.predict(X_val)\n",
        "model_eval = evaluate_model(y_val, y_pred)\n",
        "print('Evaluate metrics on Validation Set:')\n",
        "model_eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T17:10:47.637Z",
          "start_time": "2025-04-02T17:10:32.419639Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 866
        },
        "id": "K0iD-S2n527R",
        "outputId": "2e1c4c55-3d9b-4667-f277-828bb0eca991"
      },
      "outputs": [],
      "source": [
        "plot_regression_line(model=modelPoly, X=X_train, y_true=y_train, name_plot=\"Training Set: \")\n",
        "plot_regression_line(model=modelPoly, X=X_val, y_true=y_val, name_plot=\"Validation Set: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T17:10:47.852400Z",
          "start_time": "2025-04-02T17:10:47.838007Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "kUgoAVoQ527R",
        "outputId": "57b6c785-8d29-4ea3-8663-b076c2bf6d49"
      },
      "outputs": [],
      "source": [
        "display_equation(model=modelPoly, feature_names = X_train.columns.tolist(), top_k=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kgKBpV0527R"
      },
      "source": [
        "#### c) Train by PolynomialRegressionGD (using Gradient Descent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T17:14:04.229515Z",
          "start_time": "2025-04-02T17:10:47.956404Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTWGDiWz527R",
        "outputId": "f588867d-e215-4e8e-e014-303812419926"
      },
      "outputs": [],
      "source": [
        "modelPoly = PolynomialRegressionGD(\n",
        "        degree=3,\n",
        "        learning_rate=0.01,\n",
        "        n_iterations=5000,\n",
        "        batch_size=32,\n",
        "        random_state=42\n",
        ")\n",
        "\n",
        "modelPoly.fit(X_train, y_train, verbose=True)\n",
        "print(\"Model trained successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T17:14:13.114962Z",
          "start_time": "2025-04-02T17:14:05.363121Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "T4fBG73D527S",
        "outputId": "d8a898b4-d739-4256-87c6-a5bf0af8be3c"
      },
      "outputs": [],
      "source": [
        "# On Traning Set\n",
        "y_pred = modelPoly.predict(X_train)\n",
        "model_eval = evaluate_model(y_train, y_pred)\n",
        "print('Evaluate metrics on Training Set:')\n",
        "model_eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T17:14:15.631923Z",
          "start_time": "2025-04-02T17:14:14.238753Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "yrllCgAa527S",
        "outputId": "0256ef59-9a8c-4ba8-ec2a-e131a6fdeefa"
      },
      "outputs": [],
      "source": [
        "# On Validation Set\n",
        "y_pred = modelPoly.predict(X_val)\n",
        "model_eval = evaluate_model(y_val, y_pred)\n",
        "print('Evaluate metrics on Validation Set:')\n",
        "model_eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T17:14:50.978317Z",
          "start_time": "2025-04-02T17:14:17.013703Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "xnS-Tn9E527S",
        "outputId": "2b408ec6-ebae-4428-d782-7394ecf17200"
      },
      "outputs": [],
      "source": [
        "modelPoly.plot_learning_curve()\n",
        "plot_regression_line(model=modelPoly, X=X_train, y_true=y_train, name_plot=\"Training Set: \")\n",
        "plot_regression_line(model=modelPoly, X=X_val, y_true=y_val, name_plot=\"Validation Set: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T17:14:51.374281Z",
          "start_time": "2025-04-02T17:14:51.341788Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "dIj1_sQa527S",
        "outputId": "4181c19d-c163-4a61-b70b-dad82b32837b"
      },
      "outputs": [],
      "source": [
        "display_equation(model=modelPoly, feature_names=X_train.columns.tolist(), top_k=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toxLtis_SWx7"
      },
      "source": [
        "## Part IX: Linear Regression model with PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdeM1dWISWx7"
      },
      "source": [
        "#### IX.1 Data and Heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:20:45.639754Z",
          "start_time": "2025-04-02T09:20:45.625802Z"
        },
        "id": "vq4xXLapSWx8"
      },
      "outputs": [],
      "source": [
        "train_data_PCA= copy.deepcopy(train_data_after_encoding)\n",
        "validation_data_PCA = copy.deepcopy(validation_data_after_encoding)\n",
        "\n",
        "train_data_PCA = train_data_PCA.data # train_data_PCA (after encoding)\n",
        "validation_data_PCA = validation_data_PCA.data # validation_data_PCA (after encoding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:20:48.862223Z",
          "start_time": "2025-04-02T09:20:46.425760Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "T1FkAIbbSWx9",
        "outputId": "ade2fca9-2601-4c83-91e6-1bc12a5dad28"
      },
      "outputs": [],
      "source": [
        "# Tính ma trận tương quan\n",
        "correlation_matrix = train_data_PCA.corr().to_numpy()\n",
        "labels = train_data_PCA.columns\n",
        "\n",
        "# Vẽ heatmap bằng Matplotlib\n",
        "plt.figure(figsize=(23, 10))  # Điều chỉnh kích thước\n",
        "plt.imshow(correlation_matrix, cmap=\"coolwarm\", interpolation=\"nearest\")\n",
        "plt.colorbar()  # Thanh màu\n",
        "\n",
        "# Thêm giá trị số vào mỗi ô\n",
        "num_vars = len(correlation_matrix)\n",
        "for i in range(num_vars):\n",
        "    for j in range(num_vars):\n",
        "        plt.text(j, i, f\"{correlation_matrix[i, j]:.2f}\", ha=\"center\", va=\"center\", color=\"black\")\n",
        "\n",
        "# Đặt tên trục\n",
        "plt.xticks(np.arange(num_vars), labels, rotation=90)\n",
        "plt.yticks(np.arange(num_vars), labels)\n",
        "\n",
        "plt.title(\"Heatmap - Ma trận tương quan\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YrEfjGjSWx9"
      },
      "source": [
        "Nhận xét: Từ ma trận heatmap (Ma trận tương quan) ta thấy những bộ tương quan sau có thể giảm số chiều về 1\n",
        "\n",
        "['Seating Capacity', 'Length', 'Height', 'Width', 'Fuel Tank Capacity']\n",
        "\n",
        "['Engine', 'Max Power', 'Max Torque', 'Drivetrain']\n",
        "\n",
        "['rpm at Max Power', 'rpm at Max Torque']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47HstvLjSWx9"
      },
      "source": [
        "#### IX.2 PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:20:49.313368Z",
          "start_time": "2025-04-02T09:20:49.246217Z"
        },
        "id": "zRbrtwgF527T"
      },
      "outputs": [],
      "source": [
        "def pca(X, num_components):\n",
        "    \"\"\"\n",
        "    Áp dụng PCA để trích xuất số lượng thành phần chính mong muốn.\n",
        "    \"\"\"\n",
        "    X_meaned = X - np.mean(X, axis=0)  # Chuẩn hóa dữ liệu\n",
        "    covariance_matrix = np.cov(X_meaned, rowvar=False)  # Ma trận hiệp phương sai\n",
        "    eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)  # Eigen decomposition\n",
        "    sorted_indices = np.argsort(eigenvalues)[::-1]  # Sắp xếp giá trị riêng giảm dần\n",
        "    eigenvectors_sorted = eigenvectors[:, sorted_indices]  # Lấy vector riêng theo thứ tự\n",
        "    principal_components = eigenvectors_sorted[:, :num_components]  # Chọn số thành phần chính\n",
        "    X_pca = np.dot(X_meaned, principal_components)  # Biến đổi dữ liệu sang PCA\n",
        "    return X_pca, principal_components  # Trả về dữ liệu PCA + vector chính\n",
        "\n",
        "# Số thành phần chính\n",
        "num_components = 1\n",
        "\n",
        "# --- BƯỚC 1: ÁP DỤNG PCA TRÊN TRAIN ---\n",
        "columns_to_pca_groups = {\n",
        "    'hs': ['Seating Capacity', 'Length', 'Height', 'Width', 'Fuel Tank Capacity'],\n",
        "    'cs': ['Engine', 'Max Power', 'Max Torque', 'Drivetrain'],\n",
        "    'kt': ['rpm at Max Power', 'rpm at Max Torque']\n",
        "}\n",
        "\n",
        "pca_results = {}\n",
        "\n",
        "for group, columns in columns_to_pca_groups.items():\n",
        "    for col in columns:\n",
        "        if col not in train_data_PCA.columns:\n",
        "            raise KeyError(f\"Cột '{col}' không tồn tại trong train_data_after_normalization.\")\n",
        "\n",
        "    X_pca_input = train_data_PCA[columns].values\n",
        "    X_pca_transformed, principal_components = pca(X_pca_input, num_components)\n",
        "\n",
        "    pca_results[group] = {\n",
        "        'transformed': X_pca_transformed,\n",
        "        'components': principal_components\n",
        "    }\n",
        "\n",
        "# Tạo dataframe chứa PCA từ train\n",
        "pca_df1 = pd.DataFrame(pca_results['hs']['transformed'], columns=['PCA_hs'])\n",
        "pca_df2 = pd.DataFrame(pca_results['cs']['transformed'], columns=['PCA_cs'])\n",
        "pca_df3 = pd.DataFrame(pca_results['kt']['transformed'], columns=['PCA_kt'])\n",
        "\n",
        "# Xóa các cột gốc trong train\n",
        "train_data_after_normalization1 = train_data_PCA.drop(\n",
        "    columns=sum(columns_to_pca_groups.values(), [])\n",
        ")\n",
        "\n",
        "# Gộp train với các thành phần PCA\n",
        "train_data_after_normalization1 = pd.concat([train_data_after_normalization1, pca_df1, pca_df2, pca_df3], axis=1)\n",
        "\n",
        "# --- BƯỚC 2: ÁP DỤNG PCA TRÊN VALIDATION ---\n",
        "val_data_after_normalization1 = validation_data_PCA.copy()\n",
        "\n",
        "for group, columns in columns_to_pca_groups.items():\n",
        "    for col in columns:\n",
        "        if col not in validation_data_PCA.columns:\n",
        "            raise KeyError(f\"Cột '{col}' không tồn tại trong val_data_after_normalization.\")\n",
        "\n",
        "    X_val_input = validation_data_PCA[columns].values\n",
        "    X_val_meaned = X_val_input - np.mean(train_data_PCA[columns].values, axis=0)  # Dùng mean từ train\n",
        "    X_val_pca = np.dot(X_val_meaned, pca_results[group]['components'])  # Transform bằng PCA từ train\n",
        "\n",
        "    val_data_after_normalization1[f'PCA_{group}'] = X_val_pca\n",
        "\n",
        "# Xóa các cột gốc trong validation\n",
        "val_data_after_normalization1.drop(columns=sum(columns_to_pca_groups.values(), []), inplace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cUbhg-KSWx-"
      },
      "source": [
        "#### IX.3 Split data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:20:50.064986Z",
          "start_time": "2025-04-02T09:20:50.035369Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1kaS35tSWx-",
        "outputId": "7fd60c7d-6700-4c6a-bda8-46e3fdaba821"
      },
      "outputs": [],
      "source": [
        "train_data_v2 = copy.deepcopy(train_data_after_normalization1)\n",
        "validation_data_v2 = copy.deepcopy(val_data_after_normalization1)\n",
        "\n",
        "numeric_features = ['Make', 'Model', 'Year', 'Kilometer', 'Fuel Type', 'Transmission',\n",
        "                    'Location', 'Color', 'Owner', 'Seller Type', 'PCA_hs', 'PCA_cs', 'PCA_kt']\n",
        "\n",
        "for col in numeric_features:\n",
        "    mean = train_data_v2[col].mean()\n",
        "    std = train_data_v2[col].std()\n",
        "\n",
        "    if std == 0:\n",
        "        std = 1\n",
        "\n",
        "    train_data_v2[col] = (train_data_v2[col] - mean) / std\n",
        "    validation_data_v2[col] = (validation_data_v2[col] - mean) / std\n",
        "\n",
        "\n",
        "X_train_v2 = train_data_v2.drop(columns=['Price'])\n",
        "y_train_v2 = train_data_v2['Price']\n",
        "\n",
        "X_val_v2 = validation_data_v2.drop(columns=['Price'])\n",
        "y_val_v2 = validation_data_v2['Price']\n",
        "\n",
        "print(\"Shape of validation_data_v2 after scaling:\", validation_data_v2.shape)\n",
        "print(\"Shape of validation_data_v2 after scaling:\", train_data_v2.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm161M9qSWx-"
      },
      "source": [
        "####  IX.4 Model with PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8ObuV3_SWx-"
      },
      "source": [
        "##### Model Multiple Linear Regression with PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:20:50.921702Z",
          "start_time": "2025-04-02T09:20:50.588991Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "58Tcop-eSWx-",
        "outputId": "aa7f92ce-79f7-43a7-b69e-0565639e9414"
      },
      "outputs": [],
      "source": [
        "class MultipleLinearRegressionPCA:\n",
        "    def __init__(self, learning_rate=0.01, n_epochs=1000, batch_size=32):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_epochs = n_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.loss_history = []\n",
        "\n",
        "    def compute_loss(self, y_true, y_pred):\n",
        "        return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "    def forward(self, X):\n",
        "        return np.dot(X, self.weights) + self.bias\n",
        "\n",
        "    def backward(self, X, y_true, y_pred):\n",
        "        dw = -2 / len(X) * np.dot(X.T, (y_true - y_pred))\n",
        "        db = -2 / len(X) * np.sum((y_true - y_pred))\n",
        "        return dw, db\n",
        "\n",
        "    def fit(self, X_train, y_train):\n",
        "        if isinstance(X_train, (pd.DataFrame, pd.Series)):\n",
        "             X_train = X_train.to_numpy()\n",
        "        if isinstance(y_train, (pd.DataFrame, pd.Series)):\n",
        "            y_train = y_train.to_numpy()\n",
        "\n",
        "        n_samples, n_features = X_train.shape\n",
        "        self.weights = np.zeros((n_features, 1))\n",
        "        self.bias = 0\n",
        "\n",
        "        if y_train.ndim == 1:\n",
        "            y_train = y_train.reshape(-1, 1)\n",
        "\n",
        "        best_loss = float('inf')\n",
        "        best_weights = None\n",
        "        best_bias = None\n",
        "        self.loss_history = []\n",
        "\n",
        "        for epoch in range(self.n_epochs):\n",
        "            idx = np.random.permutation(n_samples)\n",
        "            X_train = X_train[idx]\n",
        "            y_train = y_train[idx]\n",
        "\n",
        "            for i in range(0, n_samples, self.batch_size):\n",
        "                X_batch = X_train[i:i + self.batch_size]\n",
        "                y_batch = y_train[i:i + self.batch_size]\n",
        "\n",
        "                y_pred = self.forward(X_batch)\n",
        "                dw, db = self.backward(X_batch, y_batch, y_pred)\n",
        "\n",
        "                self.weights -= self.learning_rate * dw\n",
        "                self.bias -= self.learning_rate * db\n",
        "\n",
        "            # Tính loss trên toàn bộ tập train\n",
        "            y_train_pred = self.forward(X_train)\n",
        "            loss = self.compute_loss(y_train, y_train_pred)\n",
        "            self.loss_history.append(loss)\n",
        "\n",
        "            if loss < best_loss:\n",
        "                best_loss = loss\n",
        "                best_weights = self.weights.copy()\n",
        "                best_bias = self.bias\n",
        "\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                print(f\"Epoch {epoch+1}, loss: {loss:.4f}\")\n",
        "\n",
        "        self.weights = best_weights\n",
        "        self.bias = best_bias\n",
        "        print(f\"Best model found with loss = {best_loss:.4f}\")\n",
        "\n",
        "        return y_train ,y_train_pred\n",
        "\n",
        "    def evaluate(self, X_test, y_test, isTest = False):\n",
        "        if isinstance(X_test, pd.DataFrame) or isinstance(X_test, pd.Series):\n",
        "            X_test = X_test.to_numpy()\n",
        "        if isinstance(y_test, pd.DataFrame) or isinstance(y_test, pd.Series):\n",
        "            y_test = y_test.to_numpy()\n",
        "\n",
        "        if y_test.ndim == 1:\n",
        "            y_test = y_test.reshape(-1, 1)\n",
        "\n",
        "        y_test_pred = self.forward(X_test)\n",
        "        test_loss = self.compute_loss(y_test, y_test_pred)\n",
        "        if (not isTest): print(f\"Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "        weights = self.weights.flatten()\n",
        "        bias = self.bias\n",
        "        terms = [f\"{w:.2f} * x{i+1}\" for i, w in enumerate(weights)]\n",
        "        equation = \" + \".join(terms) + f\" + {bias:.2f}\"\n",
        "\n",
        "        if (not isTest): print(\"\\nPhương trình hồi quy tuyến tính:\")\n",
        "        if (not isTest): print(f\"y = {equation}\")\n",
        "\n",
        "        # Vẽ đồ thị dự đoán\n",
        "        if (not isTest): self.plot_actual_vs_predicted(y_test, y_test_pred)\n",
        "        if (not isTest): self.plot_residuals(y_test, y_test_pred)\n",
        "\n",
        "        return test_loss, y_test_pred\n",
        "\n",
        "    def mean_absolute_percentage_error(self, y_true, y_pred):\n",
        "        return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "    def plot_loss_history(self):\n",
        "        plt.plot(self.loss_history)\n",
        "        plt.xlabel(\"Epochs\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.title(\"Loss History\")\n",
        "        plt.show()\n",
        "\n",
        "    def plot_actual_vs_predicted(self, y_true, y_pred):\n",
        "        plt.scatter(y_true, y_pred, alpha=0.5)\n",
        "        plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--')\n",
        "        plt.xlabel(\"Actual Values\")\n",
        "        plt.ylabel(\"Predicted Values\")\n",
        "        plt.title(\"Actual vs Predicted\")\n",
        "        plt.show()\n",
        "\n",
        "    def plot_residuals(self, y_true, y_pred):\n",
        "        residuals = y_true - y_pred\n",
        "        plt.scatter(y_pred, residuals, alpha=0.5)\n",
        "        plt.axhline(y=0, color='red', linestyle='--')\n",
        "        plt.xlabel(\"Predicted Values\")\n",
        "        plt.ylabel(\"Residuals\")\n",
        "        plt.title(\"Residual Plot\")\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "# Train and Evaluate Model\n",
        "print(\"\\nEvaluation metrics on Training Set:\")\n",
        "modelPCA = MultipleLinearRegressionPCA(learning_rate=0.001, n_epochs=200)\n",
        "y_train , y_train_pred = modelPCA.fit(X_train_v2, y_train_v2)\n",
        "modelPCA.plot_loss_history()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:20:51.573374Z",
          "start_time": "2025-04-02T09:20:51.563449Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "4ZiRSJnGSWx_",
        "outputId": "67376bcb-1cbe-4864-b198-1e5e3a1d01a3"
      },
      "outputs": [],
      "source": [
        "print(\"\\nEvaluation metrics on Training Set:\")\n",
        "evaluate_model(y_train, y_train_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:20:52.580237Z",
          "start_time": "2025-04-02T09:20:52.206962Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "joJxi7i-SWx_",
        "outputId": "521a7a9f-64cc-4abf-dc2f-4a7e9d2a5003"
      },
      "outputs": [],
      "source": [
        "test_loss, y_test_pred = modelPCA.evaluate(X_val_v2, y_val_v2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:20:54.093180Z",
          "start_time": "2025-04-02T09:20:54.083389Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "tVJeS_pjSWx_",
        "outputId": "29976df2-a1bd-416c-af0f-2d8e8da753ef"
      },
      "outputs": [],
      "source": [
        "print(\"\\nEvaluation metrics on Validation Set:\")\n",
        "evaluate_model(y_val_v2, y_test_pred.reshape(-1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycPukIHZSWx_"
      },
      "source": [
        "Actual vs Predicted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:20:54.679497Z",
          "start_time": "2025-04-02T09:20:54.673299Z"
        },
        "id": "_hyQXH2tSWx_"
      },
      "outputs": [],
      "source": [
        "# Sao chép dữ liệu\n",
        "y_test_pred_copy = np.copy(y_test_pred)\n",
        "y_val_v2_copy = np.copy(y_val_v2)\n",
        "\n",
        "# Chuyển về 1D nếu cần\n",
        "y_test_pred_copy = y_test_pred_copy.ravel()\n",
        "y_val_v2_copy = y_val_v2_copy.ravel()\n",
        "\n",
        "# Tạo DataFrame\n",
        "df_PCA1 = pd.DataFrame({'Predicted': y_test_pred_copy, 'Actual': y_val_v2_copy})\n",
        "\n",
        "# Chuyển về kiểu int nếu không gây lỗi\n",
        "df_PCA1 = df_PCA1.astype(int, errors='ignore')  # Dùng 'ignore' để tránh lỗi nếu có giá trị không chuyển được"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:20:55.444915Z",
          "start_time": "2025-04-02T09:20:55.436574Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "19PBNmaVSWx_",
        "outputId": "93d0c231-ae84-40c5-a94b-f500f92d69d7"
      },
      "outputs": [],
      "source": [
        "df_PCA1.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srPT08El527a"
      },
      "source": [
        "## Part X: Evaluate on Test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:20:56.537089Z",
          "start_time": "2025-04-02T09:20:56.532908Z"
        },
        "id": "gSGTF1_f527a"
      },
      "outputs": [],
      "source": [
        "file_test_path = './data/train.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:20:57.271950Z",
          "start_time": "2025-04-02T09:20:57.151295Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "edw-3Qeh527b",
        "outputId": "7270f91a-627b-4492-ff6e-5e000f519c1b"
      },
      "outputs": [],
      "source": [
        "def evaluate_test_set_simpleLR(file_test_path, weight_simpleLR):\n",
        "    data = DataProcessor(file_test_path)\n",
        "    data.load_data(isTest = True)\n",
        "    data.clean_data_test_set(info_fill_na)\n",
        "    data.transform_data()\n",
        "    data.encode_data_test_set(encoding_information)\n",
        "    data.normalize_data_test_set(range_vals, min_vals)\n",
        "    test_data = data.data\n",
        "\n",
        "    X = test_data.drop(columns=['Price'])\n",
        "    y = test_data['Price']\n",
        "\n",
        "    beta0 = weight_simpleLR[0]\n",
        "    beta1 = weight_simpleLR[1]\n",
        "    print(\"\\nEvaluation metrics on Test Set (Simple LR):\")\n",
        "    y_pred = beta0 + beta1 * X['Model']\n",
        "\n",
        "    model_eval = evaluate_model(y, y_pred)\n",
        "    return model_eval\n",
        "\n",
        "evaluate_test_set_simpleLR(file_test_path, weight_simpleLR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:20:58.269376Z",
          "start_time": "2025-04-02T09:20:58.261510Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "DQI6NpvZ527e",
        "outputId": "69f79538-4b19-4600-dbfa-fab5a16ccfba"
      },
      "outputs": [],
      "source": [
        "def evaluate_test_set_multipleLR(file_test_path):\n",
        "    data = DataProcessor(file_test_path)\n",
        "    data.load_data(isTest = True)\n",
        "    data.clean_data_test_set(info_fill_na)\n",
        "    data.transform_data()\n",
        "    data.encode_data_test_set(encoding_information)\n",
        "    data.normalize_data_test_set(range_vals, min_vals)\n",
        "    test_data = data.data\n",
        "\n",
        "    X = test_data.drop(columns=['Price'])\n",
        "    y = test_data['Price']\n",
        "    print(\"\\nEvaluation metrics on Test Set (Multiple LR):\")\n",
        "    y_pred = model_multi.predict(X)\n",
        "    model_eval = evaluate_model(y, y_pred)\n",
        "    return model_eval\n",
        "\n",
        "evaluate_test_set_multipleLR(file_test_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T17:07:19.426942Z",
          "start_time": "2025-04-02T17:07:00.500625Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "X0_KNie1527e",
        "outputId": "07613b85-e294-4b49-fabf-c4327b24610c"
      },
      "outputs": [],
      "source": [
        "def evaluate_test_set_polynomialLR(file_test_path):\n",
        "    data = DataProcessor(file_test_path)\n",
        "    data.load_data(isTest = True)\n",
        "    data.clean_data_test_set(info_fill_na)\n",
        "    data.transform_data()\n",
        "    data.encode_data_test_set(encoding_information)\n",
        "    data.normalize_data_test_set(range_vals, min_vals)\n",
        "    test_data = data.data\n",
        "\n",
        "    X = test_data.drop(columns=['Price'])\n",
        "    y = test_data['Price']\n",
        "\n",
        "    y_pred = modelPoly.predict(X)\n",
        "\n",
        "    # display_equation(model=modelPoly, feature_names=X_train.columns.tolist(), top_k=10)\n",
        "\n",
        "    # plot_regression_line(model=modelPoly, X=X, y_true=y, name_plot=\"Test Set: \")\n",
        "\n",
        "    model_eval = evaluate_model(y, y_pred)\n",
        "    print(\"\\nEvaluation metrics on Test Set (Polynomial LR):\")\n",
        "    return model_eval\n",
        "\n",
        "evaluate_test_set_polynomialLR(file_test_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-02T09:21:19.689436Z",
          "start_time": "2025-04-02T09:21:19.270200Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "XhYklSXw527f",
        "outputId": "f96f5269-54b8-4275-b27b-b26bced24d6e"
      },
      "outputs": [],
      "source": [
        "def evaluate_test_set_PCALR(file_test_path, train_data_after_normalization1):\n",
        "    data = DataProcessor(file_test_path)\n",
        "    data.load_data(isTest = True)\n",
        "    data.clean_data_test_set(info_fill_na)\n",
        "    data.transform_data()\n",
        "    data.encode_data_test_set(encoding_information)\n",
        "    test = data.data\n",
        "\n",
        "    columns_to_pca_groups = {\n",
        "        'hs': ['Seating Capacity', 'Length', 'Height', 'Width', 'Fuel Tank Capacity'],\n",
        "        'cs': ['Engine', 'Max Power', 'Max Torque', 'Drivetrain'],\n",
        "        'kt': ['rpm at Max Power', 'rpm at Max Torque']\n",
        "    }\n",
        "\n",
        "    test_after_encode = test.copy()\n",
        "\n",
        "    for group, columns in columns_to_pca_groups.items():\n",
        "        for col in columns:\n",
        "            if col not in test.columns:\n",
        "                raise KeyError(f\"Cột '{col}' không tồn tại trong val_data_after_normalization.\")\n",
        "\n",
        "        X_val_input = test[columns].values\n",
        "        X_val_meaned = X_val_input - np.mean(train_data_PCA[columns].values, axis=0)  # Dùng mean từ train\n",
        "        X_val_pca = np.dot(X_val_meaned, pca_results[group]['components'])  # Transform bằng PCA từ train\n",
        "\n",
        "        test_after_encode[f'PCA_{group}'] = X_val_pca\n",
        "\n",
        "    # Xóa các cột gốc trong validation\n",
        "    test_after_encode.drop(columns=sum(columns_to_pca_groups.values(), []), inplace=True)\n",
        "\n",
        "\n",
        "    numeric_features = [\n",
        "        'Make', 'Model', 'Year', 'Kilometer', 'Fuel Type', 'Transmission',\n",
        "        'Location', 'Color', 'Owner', 'Seller Type', 'PCA_hs', 'PCA_cs', 'PCA_kt'\n",
        "    ]\n",
        "    for col in numeric_features:\n",
        "        mean = train_data_after_normalization1[col].mean()\n",
        "        x= mean\n",
        "        std = train_data_after_normalization1[col].std()\n",
        "        y= std\n",
        "\n",
        "        if std == 0:\n",
        "            std = 1\n",
        "\n",
        "        test_after_encode[col] = (test_after_encode[col] - mean) / std\n",
        "\n",
        "\n",
        "    X_test = test_after_encode.drop(columns=['Price'])\n",
        "    y_test = test_after_encode['Price']\n",
        "\n",
        "    test_loss, y_test_pred = modelPCA.evaluate(X_test, y_test, True)\n",
        "    model_eval = evaluate_model(y_test, y_test_pred.reshape(-1))\n",
        "\n",
        "    print(\"\\nEvaluation metrics on Test Set (PCA LR):\")\n",
        "    return model_eval\n",
        "\n",
        "evaluate_test_set_PCALR(file_test_path, train_data_after_normalization1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "pQnQG-G5SWxT",
        "ieMN6vN_SWxa",
        "so357rYrSWxb",
        "dRxWR0zbSWxd",
        "ehyyfh6vSWxn",
        "dtPzjqSYSWxp",
        "GEG1muruSWxp",
        "TUfhmMVmSWxq",
        "czuAkOkoSWxs",
        "9M4hCC15R6ri",
        "NnOQNENmSWxw",
        "UXPH4IIQSWx0",
        "VsHRSz22SWx1",
        "qzLssHMbSQx_",
        "IcJPXafZTZxf",
        "FJj8Gei-vfjg",
        "mYEqRqnwSWx3",
        "AsCG0a_6SWx5",
        "lZzU6fx8SWx5",
        "toxLtis_SWx7"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
